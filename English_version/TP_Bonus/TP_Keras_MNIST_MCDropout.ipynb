{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yuxdaL8ExK8l"
   },
   "source": [
    "# TP Programming with Keras - MNIST problem, confidence level"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0SBMXtaWxK8m"
   },
   "source": [
    "In this training session, we associate a confidence level to our predictions by using the MC-Dropout (MC = Monte-Carlo) method. This method consists in keeping the Dropout operation active during the test, and we use the property of randomness of Dropout to obtain a variability on the output of the network: a high variability implies a low confidence level (and vice versa).\n",
    "\n",
    "In this practice session, some cells must be filled according to the instructions. They are identified by the word **Exercise**. You will perform the **Verifications** yourselves in most cases, by watching if the algorithm correctly works and converges."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "aSguuu1vxK8n"
   },
   "source": [
    "Below we import the required libraries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "WeOG9mTVxK8n"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow.keras as keras\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wxYTKsUixK8o"
   },
   "source": [
    "## Data definition"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CO-hY331xK8o"
   },
   "source": [
    "The following cell loads the MNIST data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "bV3nZ5Q2xK8o",
    "outputId": "39e1de9b-fc4a-46a1-cce1-d1229bb67ccd"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/mnist.npz\n",
      "\u001b[1m11490434/11490434\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 0us/step\n"
     ]
    }
   ],
   "source": [
    "#DO NOT CHANGE\n",
    "\n",
    "(X_train, Y_train), (X_test, Y_test) = keras.datasets.mnist.load_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TUVGAgMDxK8o"
   },
   "source": [
    "**Exercise**: Apply data normalization (division by 255) and change the output data into categorial vectors (one hot encoding with keras.utils.categorical)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "3BGKBVGnxK8p"
   },
   "outputs": [],
   "source": [
    "#TO DO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TLGsPJgQxK8p"
   },
   "source": [
    "**Exercise**: Adapt the dimension of X_train and X_test in order to use 2D convolution layers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "jvfD_kt0xK8p"
   },
   "outputs": [],
   "source": [
    "#TO DO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9x5cMDHLxK8q"
   },
   "source": [
    "## Keras model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0u7O7bgAxK8q"
   },
   "source": [
    "### Model creation with convolutional layers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Kq1H4O6TxK8q"
   },
   "source": [
    "**Exercise**: Create a Keras model with name \"my_model\".\n",
    "\n",
    "**Specific instructions**:\n",
    "- Use the following format: it is not a sequential format anymore because we need to introduce a specific option to keep the Dropout active during at test time.\n",
    "- Use the following format:\n",
    "  - x = your_layer_1(arguments)(x)\n",
    "  - x = your_layer_2(arguments)(x)\n",
    "  - ....\n",
    "  - outputs = your_final_layer(arguments)(x)\n",
    "- For the Dropout layers, add with the argument x a keyword \"training = True\" to keep the Dropout active at prediction time. Do not use Batch_Normalization layer before or after a Dropout layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "-aBfHoRBxK8q"
   },
   "outputs": [],
   "source": [
    "inputs = keras.layers.Input((28,28,1))\n",
    "\n",
    "x = keras.layers.Conv2D(#TO DO)(inputs)\n",
    "x = keras.layers.BatchNormalization()(x)\n",
    "    \n",
    "#TO DO\n",
    "\n",
    "outputs =#TO DO\n",
    "\n",
    "my_model = keras.models.Model(inputs,outputs)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "oa7HndFLxK8r"
   },
   "source": [
    "**Exercise**: Display your architecture by calling my_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Wu0c93ePxK8r",
    "outputId": "33885a1d-1132-43ce-dc46-0f20203cc666"
   },
   "outputs": [],
   "source": [
    "#TO DO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Ue7YpWAHxK8s"
   },
   "source": [
    "### Model compilation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4xKL6LAbxK8s"
   },
   "source": [
    "**Exercise**: Compile your model and choose an optimizer. Use adapted loss function and metrics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "UiddXvmyxK8s",
    "outputId": "387c772f-a709-4d6f-d9f2-a0defcfdfedb"
   },
   "outputs": [],
   "source": [
    "#TO DO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DqjUy2iIxK8t"
   },
   "source": [
    "### Early stopping"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NtT3xYc0xK8t"
   },
   "source": [
    "**Exercise**: Define an early stopping procedure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "8sM9js1fxK8t"
   },
   "outputs": [],
   "source": [
    "#TO DO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5SDIMh6GxK8t"
   },
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "783ccb8YxK8u"
   },
   "source": [
    "**Exercise**: Run the training as usual."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "WoBEyosmxK8u",
    "outputId": "fc95400d-ccef-4446-9a73-c928f38e5558",
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "learning = #TO DO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "s2mNqp0IxK8u"
   },
   "source": [
    "**Verification**: The loss function should decrease and the accuracy should increase. Same thing for the validation loss."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Ml3lOdrqxK8u"
   },
   "source": [
    "**Exercise**: Plot the evolution of the loss function, and the evolution of the accuracy, for the training set and the validation set. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "4RHwBlrHxK8u",
    "outputId": "b5d31e87-7dce-4575-d744-d16e491b637e",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#TO DO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WgYO_f6jxK8v"
   },
   "source": [
    "## Predicting with your model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CtkAHEzLxK8v"
   },
   "source": [
    "**Exercise**: Pick randomly an example and display its prediction. Run the prediction several times: you will see that the prediction is not always the same for this example. Do not use my_model.predict (it disables Dropout in the most recent versions of TensorFlow). Instead, apply the model directly to your example: my_model(exemple)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "sObgdz-wxK8v",
    "outputId": "89d02165-f8f8-4841-c9c9-5d165059adad"
   },
   "outputs": [],
   "source": [
    "#TO DO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2kcN3qiB0T0J"
   },
   "source": [
    "We will characterize the variability of the predictions. To do so, we use the information theory to build adapted metrics in order to estimate these uncertainties (self-evaluated by the network).\n",
    "\n",
    "For one specific prediction, we characterize the uncertainty of this prediction, based on the probabilities associated to each class. The idea is the following: if the prediction gives a high probability for one class and low probabilities for the other classes, the prediction is \"certain\". On the contrary, if the probability is low for every class, the prediction is not \"sure\".\n",
    "\n",
    "This notion can be quantified by **Shannon entropy**, defined by:\n",
    "\n",
    "\\begin{equation}\n",
    "\\mathcal{H}(\\hat{Y}) = -\\sum_{i = 1}^{K} \\hat{y_i}\\log(\\hat{y_i}) \n",
    "\\end{equation}\n",
    "\n",
    "In this equation, the index $i$ corresponds to the classes, $\\hat{y}$ is the prediction.\n",
    "\n",
    "**Exercise**: Complete the following function to code the Shannon entropy. Consider that $y$ is a multi-dimensional table and we want to compute the entropy along a particular axis (argument ax) which represents the classes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "8p4Bl6vU2FU6"
   },
   "outputs": [],
   "source": [
    "def shannon_entr(y,ax):\n",
    "\n",
    "  entr = #TO DO\n",
    "\n",
    "  return entr"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bGXkGYjk2EM0"
   },
   "source": [
    "**Verification**: Run the following cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "0UN7RyAG2uyu",
    "outputId": "98824af6-7925-47c0-9137-4c82c43e1279"
   },
   "outputs": [],
   "source": [
    "#DO NOT CHANGE\n",
    "\n",
    "np.random.seed(seed = 1)\n",
    "\n",
    "y_hat = np.random.rand(3,10)\n",
    "\n",
    "print(shannon_entr(y_hat,1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "m5y-R3Jf29gL"
   },
   "source": [
    "The result should be [2.84552209 2.71503273 1.79409548]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Qn-Kd0DH3D67"
   },
   "source": [
    "Now, we run several predictions for the same example. We will get a variability thanks to the Monte-Carlo Dropout. The total uncertainty will be represented by the Shannon entropy computed on the mean prediction. The uncertainty due to the inner noise of the data (aleatoric uncertainty) is given by the mean of the Sannon entropies. Finally, the uncertainty due to the variability of the different models is given by the difference between the two previous quantities (epistemic uncertainty).\n",
    "\n",
    "Mathematically speaking, it corresponds to:\n",
    "\n",
    "  - $\\mathcal{H}(\\mathbb{E}_{w}(\\hat{Y}))$ is the total uncertainty (the index $w$ means that the expectancy is computed through the variability of the weights due to the MC-Dropout)\n",
    "  - $\\mathbb{E}_{w}(\\mathcal{H}(\\hat{Y}))$ is the aleatoric uncertainty\n",
    "  - $\\mathcal{I}(\\hat{Y};w) = \\mathcal{H}(\\mathbb{E}_{w}(\\hat{Y})) - \\mathbb{E}_{w}(\\mathcal{H}(\\hat{Y}))$ is the epistemic uncertainty. This quantity is called \"mutual information\" and represents the relation between the prediction and the variability of the weights du to the MC-Dropout."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "di90u1X9xK8v"
   },
   "source": [
    "**Exercise**: Take an example and duplicate it along the axis 0 by using np.repeat."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "iSyl1100xK8v"
   },
   "outputs": [],
   "source": [
    "i = 0\n",
    "\n",
    "X_test_i = X_test[i:(i+1)]\n",
    "\n",
    "X_test_dup = #TO DO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise**: Run your model to make a prediction on this duplicated example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y_pred_dup = #TO DO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-9uDAytQxK8v"
   },
   "source": [
    "**Exercise**: Use this prediction to compute the aleatoric uncertainty. As we previously saw, the aleatoric uncertainty is the mean of Shannon entropies computed on the several predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "incert_aleat = #TO DO\n",
    "\n",
    "print(incert_aleat)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise**: Use this prediction to compute the total uncertainty. As we previously saw, the total uncertainty is the Shannon entropy computed on the mean of the predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "incert_tot = #TO DO\n",
    "\n",
    "print(incert_tot)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise**: Finally, you can compute the epistemic uncertainty, as the difference between the total uncertainty and the aleatoric uncertainty."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "lZse7-qixK8v",
    "outputId": "3eecd938-3261-4827-e887-2d95722631a2",
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "incert_epist = #TO DO\n",
    "\n",
    "print(incert_epist)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing on the whole test database"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following code duplicates the whole database a hundred of times. Firstly, it duplicates the test vector along a supplementary axis, then it produces a reshape in ordre to obtain a table with dimensions (number of examples * n_mc,image dimension), with n_mc, the number of Monte-Carlo drawing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#DO NOT CHANGE\n",
    "\n",
    "n_mc = 100\n",
    "\n",
    "X_test_tot_dup = np.expand_dims(X_test,axis = 1)\n",
    "\n",
    "X_test_tot_dup = np.repeat(X_test_tot_dup,n_mc,axis = 1)\n",
    "\n",
    "X_test_tot_dup = np.reshape(X_test_tot_dup,(n_mc*X_test.shape[0],X_test.shape[1],X_test.shape[2],X_test.shape[3]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The function below will allow you to make predictions on batches of data using my_model directly (and not my_model.predict).\n",
    "This approach helps prevent memory saturation on the machine."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#DO NOT CHANGE\n",
    "\n",
    "def predict_on_batch_with_dropout(model, data, batch_size):\n",
    "    predictions = []\n",
    "    for i in tqdm(range(0, len(data), batch_size)):\n",
    "        batch = data[i:i+batch_size]\n",
    "        batch_predictions = model(batch, training=True)\n",
    "        predictions.append(batch_predictions)\n",
    "    return np.concatenate(predictions, axis=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise**: Run your model on this duplicated database to get a prediction using the function predict_on_batch_with_dropout."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y_pred_tot = #TO DO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The size of this table of predictions is now (n_example * n_mc, 10). To compute the Shannon entropy, you must gather the predictions corresponding to the same example into the same dimension: the idea is to get a final table with shape (n_example, n_mc, 10).\n",
    "\n",
    "**Exercise**: Use the function np.reshape to get this shape."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y_pred_tot = #TO DO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise**: Compute the aleatoric uncertainty for all of the predictions. The results must be one vector with size n_examples (= 10 000).\n",
    "\n",
    "**Hint**: The main difficulty to handle is the axis on which you compute the entropy, and the axis on which you compute the mean."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "incert_aleat_tot = #TO DO\n",
    "\n",
    "print(incert_aleat_tot.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise**: Compute the total uncertainty for every prediction. Store also the vector that gives the mean of the predictions in the vector Y_mean."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y_mean = #TO DO\n",
    "\n",
    "incert_totale_tot = #TO DO\n",
    "\n",
    "print(incert_totale_tot.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise**: Finally, compute the epistemic part."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "incert_epist_tot = #TO DO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise**: Sort the example according to the value of their aleatoric uncertainty and visualize examples with the highest aleatoric uncertainty. The function np.argsort can be helpful."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "xT-QdU-fxK8w",
    "outputId": "ffacd917-f2f8-490a-83cc-7abf203e2618",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "index_sort = #TO DO\n",
    "\n",
    "index = #TO DO: stockez l'index que vous voulez visualiser dans cette variable\n",
    "\n",
    "label_pred = np.argmax(Y_mean[index])\n",
    "\n",
    "figure = plt.figure(figsize = (16,9))\n",
    "\n",
    "ax1 = plt.subplot(121)\n",
    "ax1.imshow(X_test[index,:,:],cmap = \"hot\")\n",
    "plt.title(\"Mean prediction: \" + str(label_pred) + \"\\nTrue value: \" + str(Y_test[index]))\n",
    "\n",
    "ax2 = plt.subplot(122)\n",
    "ax2.bar(np.arange(10),height = Y_mean[index],tick_label = np.arange(10))\n",
    "plt.xlabel(\"Class\")\n",
    "plt.ylabel(\"Network output\")\n",
    "plt.title(\"Aleatoric uncertainty: \" + str(incert_aleat_tot[index]) + \n",
    "          \"\\nEpistemic uncertainty: \" + str(incert_epist_tot[index])+\n",
    "          \"\\nTotal uncertainty: \" + str(incert_totale_tot[index]))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise**: Do the same exercise for the epistemic uncertainty."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "xhChSGgOxK8w",
    "outputId": "ca8e21e1-e224-492d-f77b-496b22494799"
   },
   "outputs": [],
   "source": [
    "index_sort = #TO DO\n",
    "\n",
    "index = #TO DO: stockez l'index que vous voulez visualiser dans cette variable\n",
    " \n",
    "label_pred = np.argmax(Y_mean[index])\n",
    "\n",
    "figure = plt.figure(figsize = (16,9))\n",
    "\n",
    "ax1 = plt.subplot(121)\n",
    "ax1.imshow(X_test[index,:,:],cmap = \"hot\")\n",
    "plt.title(\"Mean prediction: \" + str(label_pred) + \"\\nTrue value: \" + str(Y_test[index]))\n",
    "\n",
    "ax2 = plt.subplot(122)\n",
    "ax2.bar(np.arange(10),height = Y_mean[index],tick_label = np.arange(10))\n",
    "plt.xlabel(\"Class\")\n",
    "plt.ylabel(\"Network output\")\n",
    "plt.title(\"Aleatoric uncertainty: \" + str(incert_aleat_tot[index]) + \n",
    "          \"\\nEpistemic uncertainty: \" + str(incert_epist_tot[index])+\n",
    "          \"\\nTotal uncertainty: \" + str(incert_totale_tot[index]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Q0rCarbKxK8x"
   },
   "source": [
    "**Exercise**: Do the same exercise for the total uncertainty."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "PZBwGpnxxK8x",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "index_sort = #TO DO\n",
    "\n",
    "index =  #TO DO: stockez l'index que vous voulez visualiser dans cette variable\n",
    "\n",
    "label_pred = np.argmax(Y_mean[index])\n",
    "\n",
    "figure = plt.figure(figsize = (16,9))\n",
    "\n",
    "ax1 = plt.subplot(121)\n",
    "ax1.imshow(X_test[index,:,:],cmap = \"hot\")\n",
    "plt.title(\"Mean prediction: \" + str(label_pred) + \"\\nTrue value: \" + str(Y_test[index]))\n",
    "\n",
    "ax2 = plt.subplot(122)\n",
    "ax2.bar(np.arange(10),height = Y_mean[index],tick_label = np.arange(10))\n",
    "plt.xlabel(\"Class\")\n",
    "plt.ylabel(\"Network output\")\n",
    "plt.title(\"Aleatoric uncertainty: \" + str(incert_aleat_tot[index]) + \n",
    "          \"\\nEpistemic uncertainty: \" + str(incert_epist_tot[index])+\n",
    "          \"\\nTotal uncertainty: \" + str(incert_totale_tot[index]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can keep on studying these data, by building the histogram of uncertainties, analysing the correlation between the uncertainties and the wrong classifications of the network..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "TP_Keras_MNIST_CNN_correction.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
