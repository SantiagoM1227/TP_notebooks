{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TP Coding a neural network from scratch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This practice session aims to build your own neural network. You will code the basic functions for the forward pass computation of a neural network, and then the backpropagation algorithm. Then, you will apply your network for a learning on a XOR problem. This practice session will help you to handle the basics and the operations inside a neural network, during the training and the inference (prediction) as well. However, in the next sessions you will use dedicated libraries, such as \"Tensorflow/Keras\"; the coding is easier and faster, and the training algorithms are optimized for computation speed (exploitation of GPUs if your device is equipped).\n",
    "\n",
    "In this practice session, some cells must be filled according to the instructions. They are identified by the word **Exercise**. Some of them are followed by verification cells, that help you to verify if your code is correct or not. These cells are identified with the word **Verification**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise**: Import numpy and matplotlib.pyplot, denote them with the keyword np and plt."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Verification**: Run the next cell, a straight line should appear."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "plt.plot(np.arange(5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Forward pass computation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this first part, we create functions that help to compute the forward pass computation of a neural network."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Activation function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise**: Let's start by coding the activation functions we will use in the network. We mainly focus on the ReLU (for hidden layers) and sigmoid (for the output, our problem will be a classification problem).\n",
    "\n",
    "Code the following Python function \"activation\" that take as input a vector $Z$, a string (either \"ReLU\" or \"sigmoide\"). The function must returns a vector on which the desired activation function is applied.\n",
    "\n",
    "Recall:\n",
    "- $ReLU(Z) = max(Z,0)$\n",
    "- $sigmoide(Z) = \\frac{1}{1 + e^{-Z}}$\n",
    "\n",
    "For the sigmoid function, I advice you to favor the use of hyperbolic tangent function to do the computation: it helps avoiding numerical problems in the computation of the exponential function (for very high values for instance). The sigmoid function can be indeed computed from the tanh function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def activation(Z,fonction):\n",
    "    \n",
    "    if fonction == \"ReLU\":\n",
    "        \n",
    "        resultat = #TO BE FILLED\n",
    "    \n",
    "    if fonction == \"sigmoide\":\n",
    "        \n",
    "        resultat = #TO BE FILLED\n",
    "    \n",
    "    return resultat"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Verification**: Run the following cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#DO NOT CHANGE\n",
    "\n",
    "Z = np.array([2.,10.,-5.,0.,-100.])\n",
    "\n",
    "print(activation(Z,\"ReLU\"))\n",
    "print(activation(Z,\"sigmoide\"))"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "The expected result is:\n",
    "\n",
    "[ 2. 10. -0.  0. -0.]\n",
    "[0.88079708 0.9999546  0.00669285 0.5        0.        ]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Computation of a layer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise**: Let's consider a layer of neurons, represented by a vector $A$ (with $n \\times N$ elements that represent the $n$ neurons of the layers, and $N$ is the number of examples, since we can process several examples simultaneously!). The following layer will contain $m$ neurons, the weights between the two layers are represented by a matrix $W$, of size $m \\times n$ and the bias by a vector $b$ with size $m$.\n",
    "\n",
    "For this exercise, we do not apply any activation function. We only compute the linear opeation: $Z = W.A + b$.\n",
    "\n",
    "You must return a variable, named \"cache\", that will only contain the 4 elements $(A,W,b,Z)$. This variable can seem useless at this point, but it will be very useful for the backpropagation algorithm: if you remember, we must use the state of every layer to compute the backpropagation.\n",
    "\n",
    "**Hint**: The function np.dot should help you for the linear operation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def one_layer_linear(A,W,b):\n",
    "    \n",
    "    Z = #TO BE FILLED\n",
    "    cache = #TO BE FILLED\n",
    "    \n",
    "    return Z, cache"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Verification**: Run the following cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#DO NOT CHANGE\n",
    "\n",
    "np.random.seed(seed = 1)\n",
    "\n",
    "A = np.random.rand(3,2)\n",
    "W = np.random.rand(4,3)\n",
    "b = np.random.rand(4,1)\n",
    "\n",
    "Z, cache = one_layer_linear(A,W,b)\n",
    "\n",
    "print(Z)\n",
    "print(cache == (A,W,b,Z))"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "The expected result is:\n",
    "\n",
    "[[0.27632903 0.41566595]\n",
    " [0.52340787 0.77623274]\n",
    " [0.89012538 1.21602897]\n",
    " [1.32990003 1.62896921]]\n",
    "True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise**: We apply now the total computation, by applying the activation function. Let $g$ the activation function, $A_{prev}$ the vector which represents the output of the previous layer, $A_{new}$ the output of the current layer, thus: $A_{new} = g(W.A_{prev} + b)$. The variable \"fonction\" is a string that defines the activation function (\"ReLU\" or \"sigmoide\" in our case).\n",
    "\n",
    "The \"cache\" variable must contain the same element as the function \"one_layer_linear\".\n",
    "\n",
    "**Hint**: Use the previous function \"activation\" and \"one_layer_linear\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def one_layer_activation(A_prev,W,b,fonction):\n",
    "    \n",
    "    Z, cache = #TO BE FILLED\n",
    "    A_new = #TO BE FILLED\n",
    "        \n",
    "    return A_new, cache"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Verification**: Run the following cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#DO NOT CHANGE\n",
    "\n",
    "np.random.seed(seed = 1)\n",
    "\n",
    "A_prev = np.random.rand(3,2)\n",
    "W = np.random.rand(4,3)\n",
    "b = np.random.rand(4,1)\n",
    "\n",
    "Z, cache_old = one_layer_linear(A_prev,W,b)\n",
    "\n",
    "A_new, cache_new = one_layer_activation(A_prev,W,b,\"sigmoide\")\n",
    "print(A_new)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "The expected result is:\n",
    "\n",
    " [[0.56864601 0.60244568]\n",
    " [0.62794429 0.68486762]\n",
    " [0.70891605 0.77136397]\n",
    " [0.7908241  0.83602838]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Computation of a complete neural network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise**: Let's assume that we have a list of parameters \"List_W\" and \"List_b\" that contain all of the network parameters (weights and biases) for every layer, and an input matrix $X$ of size $n_X \\times N$, where $n_X$ is the input dimension and $N$ is the number of examples.\n",
    "\n",
    "\"List_W\" and \"List_b\" have the same number of elements (the number of layers). We also give \"List_activ\" that contains the activation function to apply (one element per layer).\n",
    "\n",
    "Code the following function that apply the whole forward pass computation, on the complete network.\n",
    "\n",
    "The \"caches\" variable is a list that will contain all of the caches computed for each layer. The variable Y_pred will contain the network output."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Hints**:\n",
    "\n",
    "- This function will be composed by a main loop that goes through every layer of the network\n",
    "- Use the function \"one_layer_activation\" for each layer\n",
    "- In order to know the number of layers, you can take the size of one of the parameter list (List_W or List_b), the function np.size will be useful\n",
    "- Use l.append(elem) to add elements elem in a list l"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def feed_forward(X,list_W,list_b,list_activations):\n",
    "    \n",
    "    A = #TO BE FILLED\n",
    "    N_couches = #TO BE FILLED (couches = layers)\n",
    "    caches = []\n",
    "    \n",
    "    for i in range(N_couches):\n",
    "    \n",
    "        #TO BE FILLED\n",
    "    \n",
    "    Y_pred = A\n",
    "    \n",
    "    return Y_pred,caches"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Verification**: Run the following cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#DO NOT CHANGE\n",
    "\n",
    "np.random.seed(seed = 2)\n",
    "\n",
    "X = np.random.rand(5,3)\n",
    "list_W = [np.random.rand(4,5)-1/2,np.random.rand(4,4)-1/2,np.random.rand(2,4)-1/2]\n",
    "list_b = [np.random.rand(4,1)-1/2,np.random.rand(4,1)-1/2,np.random.rand(2,1)-1/2]\n",
    "list_activations = [\"ReLU\",\"ReLU\",\"sigmoide\"]\n",
    "\n",
    "Y_pred,caches = feed_forward(X,list_W,list_b,list_activations)\n",
    "\n",
    "print(Y_pred)\n",
    "print(len(caches) == 3)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "The expected result is: \n",
    "\n",
    "[[0.3863283  0.38738737 0.38502977]\n",
    " [0.62732414 0.62279049 0.62796192]]\n",
    "True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loss function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We know implement the loss function we will use later. For this problem, we use the \"binary cross-entropy\" loss function, since we will address a classification problem with non-exclusive multiclasses ($K$ classes).\n",
    "\n",
    "Find below the equation of this loss function: a prediction vector $\\hat{Y}$ with $N$ examples $\\hat{y_{ik}}$ must be compared to a vector of true (expected) values $Y$ with $N$ examples $y_{ik}$ (the index $i$ represent the example $i$ and the index $k$ represents the class $k$):\n",
    "\n",
    "\\begin{equation}\n",
    "L(Y,\\hat{Y}) = - \\frac{1}{N} \\sum_{i=1}^{N} \\sum_{k=1}^{K} y_{ik} \\log(\\hat{y_{ik}}) + (1-y_{ik})\\log(1-\\hat{y_{ik}})\n",
    "\\end{equation}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise**: Complete the loss function below, where Y represents the expected values and Y_pred represents the predictions.\n",
    "\n",
    "**Hints**:\n",
    "- The functions np.sum, np.mean and np.log will be helpful"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss_function(Y,Y_pred):\n",
    "    \n",
    "    loss = #TO BE FILLED\n",
    "    \n",
    "    return loss\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Verification**: Run the following cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#DO NOT CHANGE\n",
    "\n",
    "np.random.seed(seed = 3)\n",
    "\n",
    "Y = np.array([[1,0,1],[0,1,1]])\n",
    "Y_pred = np.random.rand(2,3)\n",
    "\n",
    "print(loss_function(Y,Y_pred))"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "The expected result is: 1.3334702695881486"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Backward propagation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this section, we code the backpropagation of the error, that computes the gradients used to update the network parameters."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Linear backward"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us consider a layer $l$^. The feed-forward computation is given by: $Z^{[l]} = W^{[l]}.A^{[l-1]} + b^{[l]}$.\n",
    "\n",
    "Let us assume that we already know the derivative of the loss function according to $Z^{[l]}$ (recursively): $\\frac{\\partial L}{\\partial Z^{[l]}}$.\n",
    "\n",
    "We know compute $\\frac{\\partial L}{\\partial W^{[l]}}$, $\\frac{\\partial L}{\\partial b^{[l]}}$ and $\\frac{\\partial L}{\\partial A^{[l-1]}}$. The first two derivatives are necessary to obtain the gradient of the loss function according to the parameters of the layer $l$. The last derivative is useful for the recursion in the backpropagation algorithm.\n",
    "\n",
    "We give below the equations:\n",
    "\n",
    "\\begin{equation}\n",
    "\\frac{\\partial L}{\\partial W^{[l]}} = \\frac{\\partial L}{\\partial Z^{[l]}} \\frac{\\partial Z^{[l]}}{\\partial W^{[l]}} = \\frac{\\partial L}{\\partial Z^{[l]}} A^{[l-1]T}\n",
    "\\end{equation}\n",
    "\n",
    "\\begin{equation}\n",
    "\\frac{\\partial L}{\\partial b^{[l]}} = \\sum_{i = 1}^{N} (\\frac{\\partial L}{\\partial Z^{[l]}})_i \n",
    "\\end{equation}\n",
    "\n",
    "\\begin{equation}\n",
    "\\frac{\\partial L}{\\partial A^{[l-1]}} = \\frac{\\partial L}{\\partial Z^{[l]}} \\frac{\\partial Z^{[l]}}{\\partial A^{[l-1]}} = W^{[l]T}\\frac{\\partial L}{\\partial Z^{[l]}} \n",
    "\\end{equation}\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise**: Complete the code with the three equations in the function \"linear_backward\". As input, we take dZ that corresponds to derivative of the loss function according to Z (supposed to be known), and the cache contains (A_prev,W,b,Z), which explains why the cache (we previously defined in the feed-forward functions) is useful.\n",
    "\n",
    "**Hint**: For the sum, pay attention to the dimension along whose the operation is applied (dimension corresponding to the examples), you can use the keyword \"axis = ...\". Take care of using the keyword \"keepdims = True\" in order to compute consistant operations according to the dimension of the objects."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def linear_backward(dZ,cache):\n",
    "    \n",
    "    A_prev,W,b,Z = cache\n",
    "    \n",
    "    dW = #TO BE FILLED\n",
    "    db = #TO BE FILLED\n",
    "    dA_prev = #TO BE FILLED\n",
    "    \n",
    "    return dA_prev,dW,db"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Verification**: Run the following cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#DO NOT CHANGE\n",
    "\n",
    "np.random.seed(seed = 3)\n",
    "\n",
    "dZ = np.random.rand(3,2)\n",
    "\n",
    "A_pred = np.random.rand(4,2)\n",
    "W = np.random.rand(3,4)\n",
    "b = np.random.rand(3,1)\n",
    "Z = np.random.rand(3,2)\n",
    "\n",
    "cache =  (A_pred,W,b,Z)\n",
    "\n",
    "print(linear_backward(dZ,cache))"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "The expected result is:\n",
    "\n",
    "(array([[0.84119935, 1.00609737],\n",
    "       [0.58627548, 0.77106284],\n",
    "       [0.58203064, 0.64998031],\n",
    "       [1.20623246, 1.44921602]]), array([[0.21593072, 0.34050656, 0.33996121, 0.55475734],\n",
    "       [0.14239875, 0.24014989, 0.24205415, 0.33109807],\n",
    "       [0.29789138, 0.4410523 , 0.43613433, 0.82925743]]), array([[1.25894573],\n",
    "       [0.80173234],\n",
    "       [1.78924004]]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Add the activation function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise**: Firstly, we compute the derivative of the loss function that we use in these exercices:\n",
    "\n",
    "- For $g = ReLU$, $g'(x) = 0$ if $x < 0$ and $g'(x) = 1$ if $x \\ge 0$.\n",
    "- For $g = sigmoid$, $g'(x) = g(x)(1-g(x))$\n",
    "\n",
    "**Hints**: \n",
    "- Do not use conditions as \"if, then, else\" for the ReLU! Use directly the boolean code $Z > 0$.\n",
    "- For the sigmoid, use the activation function that you have coded in the first cells."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def derivate_activation(Z,activ):\n",
    "    \n",
    "    if activ == \"ReLU\":\n",
    "        \n",
    "        dg = #TO BE FILLED\n",
    "        \n",
    "    if activ == \"sigmoide\":\n",
    "        \n",
    "        dg = #TO BE FILLED\n",
    "    \n",
    "    return dg"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Verification**: Run the following cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#DO NOT CHANGE\n",
    "\n",
    "np.random.seed(seed = 4)\n",
    "\n",
    "Z = np.random.rand(4,2) - 1/2\n",
    "\n",
    "print(derivate_activation(Z,\"ReLU\"))\n",
    "print(derivate_activation(Z,\"sigmoide\"))"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "The expected result is:\n",
    "\n",
    "[[1. 1.]\n",
    " [1. 1.]\n",
    " [1. 0.]\n",
    " [1. 0.]]\n",
    "[[0.23684838 0.24986062]\n",
    " [0.23653961 0.24713792]\n",
    " [0.24757229 0.24502909]\n",
    " [0.23634193 0.23536043]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise**: We know add the derivative of the activation function to compute $\\frac{\\partial L}{\\partial Z^{[l]}}$ from $\\frac{\\partial L}{\\partial A^{[l]}}$ (supposed to be known by recursion, since it is compute by the function \"linear_backward\").\n",
    "\n",
    "From the following equation:\n",
    "\n",
    "\\begin{equation}\n",
    "A^{[l]} = g(Z^{[l]})\n",
    "\\end{equation}\n",
    "\n",
    "we can deduce:\n",
    "\n",
    "\\begin{equation}\n",
    "\\frac{\\partial L}{\\partial Z^{[l]}} = \\frac{\\partial L}{\\partial A^{[l]}} \\frac{\\partial A^{[l]}}{\\partial Z^{[l]}} = \\frac{\\partial L}{\\partial A^{[l]}} g'(Z^{[l]})\n",
    "\\end{equation}\n",
    "\n",
    "In the following cell, the activation function is given by the input \"activ\".\n",
    "\n",
    "**Hint**:\n",
    "- The value of $Z^{[l]}$ will be helpful, you can find it in the cache that contains (A_prev,W,b,Z).\n",
    "- Use the previous function \"linear_backward\" to find the values of dA_prev, dW and db from dZ that you will have computed, and then apply \"derivate_activation\" to get the derivative of the activation functions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def activation_backward(dA,cache,activ):\n",
    "    \n",
    "    A_prev,W,b,Z = cache\n",
    "    \n",
    "    dZ = #TO BE FILLED\n",
    "    \n",
    "    dA_prev,dW,db = #TO BE FILLED\n",
    "    \n",
    "    return dA_prev,dW,db"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Verification**: Run the following cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#DO NOT CHANGE\n",
    "\n",
    "np.random.seed(seed = 3)\n",
    "\n",
    "dA = np.random.rand(3,2)\n",
    "\n",
    "A_pred = np.random.rand(4,2)\n",
    "W = np.random.rand(3,4)\n",
    "b = np.random.rand(3,1)\n",
    "Z = np.random.rand(3,2)\n",
    "\n",
    "cache =  (A_pred,W,b,Z)\n",
    "\n",
    "print(activation_backward(dA,cache,\"sigmoide\"))"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "The expected result is:\n",
    "\n",
    "(array([[0.17487705, 0.217628  ],\n",
    "       [0.12229471, 0.16062103],\n",
    "       [0.12332169, 0.1508871 ],\n",
    "       [0.25380512, 0.32009158]]), array([[0.04316761, 0.06778688, 0.06763646, 0.11153914],\n",
    "       [0.02990346, 0.04956865, 0.04984338, 0.07145092],\n",
    "       [0.06838174, 0.10498491, 0.10439884, 0.18202664]]), array([[0.25220214],\n",
    "       [0.16993783],\n",
    "       [0.40389105]]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Finally, we apply the full chain"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We initialize the computation from the end of the network and we pass through the different layers until we reach the first layer.\n",
    "\n",
    "**Exercise**: Apply the backpropagation algorithm in the following cell. \n",
    "\n",
    "**Hints**:\n",
    "- You must initialize first the derivative of the loss function for the last layer:\n",
    "\\begin{equation}\n",
    "\\frac{\\partial L}{\\partial A^{[N_{couches}]}} = \\frac{\\partial L}{\\partial \\hat{Y}^{[l]}} = - \\frac{1}{N} (\\frac{Y}{\\hat{Y}} - \\frac{1 - Y}{1 - \\hat{Y}}) \n",
    "\\end{equation}\n",
    "\n",
    "- We build the loop by going back to the previous layers (note that we use \"reversed(range(Ncouches))\") (couches = layers)\n",
    "\n",
    "- Use the \"caches\" that are stored as a list\n",
    "\n",
    "- Finally, store the computed gradients of the parameters into two lists list_dW and list_db. Take care of inserting them at the beginning of the list (and not at the end), since the algorithm goes from the end of the network to the beginning! The function l.insert(0,var) will be helpful (l is the list, 0 is the position, beginning the list and var is variable to be inserted)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def backward_propagation(Y_pred,Y,caches,activations):\n",
    "    \n",
    "    list_dW = []\n",
    "    list_db = []\n",
    "    \n",
    "    N_couches = #TO BE FILLED\n",
    "    N = ##TO BE FILLED (number of examples)\n",
    "    Y = Y.reshape(np.shape(Y_pred))\n",
    "    \n",
    "    dA_cur = ##TO BE FILLED (initialization: derivative of the last layer)\n",
    "    \n",
    "    for l in reversed(range(N_couches)):\n",
    "\n",
    "        dA_prev,dW,db = ##TO BE FILLED\n",
    "        \n",
    "        ##TO BE FILLED: insert inside the lists list_dW, list_db\n",
    "        \n",
    "        dA_cur = ##TO BE FILLED\n",
    "    \n",
    "    return list_dW, list_db      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#DO NOT CHANGE\n",
    "\n",
    "np.random.seed(seed = 2)\n",
    "\n",
    "X = np.random.rand(5,3)\n",
    "list_W = [np.random.rand(4,5)-1/2,np.random.rand(4,4)-1/2,np.random.rand(2,4)-1/2]\n",
    "list_b = [np.random.rand(4,1)-1/2,np.random.rand(4,1)-1/2,np.random.rand(2,1)-1/2]\n",
    "list_activations = [\"ReLU\",\"ReLU\",\"sigmoide\"]\n",
    "\n",
    "Y_pred,caches = feed_forward(X,list_W,list_b,list_activations)\n",
    "\n",
    "Y = np.array([[0,1,1],[1,0,1]])\n",
    "\n",
    "print(backward_propagation(Y_pred,Y,caches,list_activations))"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "The expected result is:\n",
    "\n",
    "([array([[-0.00677534,  0.00447579,  0.01418342,  0.01319047,  0.01236515],\n",
    "       [ 0.        ,  0.        ,  0.        ,  0.        ,  0.        ],\n",
    "       [ 0.        ,  0.        ,  0.        ,  0.        ,  0.        ],\n",
    "       [ 0.01243833,  0.00607596,  0.0040221 ,  0.00898123,  0.00190816]]), array([[ 0.        ,  0.        ,  0.        ,  0.        ],\n",
    "       [ 0.04529126,  0.        ,  0.        ,  0.00732433],\n",
    "       [ 0.        ,  0.        ,  0.        ,  0.        ],\n",
    "       [-0.01746327,  0.        ,  0.        ,  0.00810363]]), array([[ 0.        , -0.00673947,  0.        , -0.01793444],\n",
    "       [ 0.        , -0.00458954,  0.        , -0.01426107]])], [array([[0.01130477],\n",
    "       [0.        ],\n",
    "       [0.        ],\n",
    "       [0.01887614]]), array([[0.        ],\n",
    "       [0.04775503],\n",
    "       [0.        ],\n",
    "       [0.00621147]]), array([[-0.28041819],\n",
    "       [-0.04064115]])])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Update of parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise**: Once the gradients are computed, you can use them to update the previous parameters and replace them by the new parameters. Code the following function that takes the list of the old parameters (list_w,list_b), the list of the gradients (list_dw,list_db) and the learning rate alpha, and returns the list of the new parameters according to the following update equations:\n",
    "\n",
    "\n",
    "\\begin{equation}\n",
    "W^{[l]} := W^{[l]} - \\alpha \\frac{\\partial L}{\\partial W^{[l]}}\n",
    "\\end{equation}\n",
    "\n",
    "\\begin{equation}\n",
    "b^{[l]} := b^{[l]} - \\alpha \\frac{\\partial L}{\\partial b^{[l]}}\n",
    "\\end{equation}\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_parameters(list_w,list_b,list_dw,list_db,alpha):\n",
    "    \n",
    "    N_couches = #TO BE FILLED\n",
    "    \n",
    "    for l in range(N_couches):\n",
    "        \n",
    "        list_w[l] = #TO BE FILLED\n",
    "        list_b[l] = #TO BE FILLED\n",
    "    \n",
    "    return list_w,list_b"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Verification**: Run the following cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#DO NOT CHANGE\n",
    "\n",
    "np.random.seed(seed = 2)\n",
    "\n",
    "X = np.random.rand(5,3)\n",
    "list_W = [np.random.rand(4,5)-1/2,np.random.rand(4,4)-1/2,np.random.rand(2,4)-1/2]\n",
    "list_b = [np.random.rand(4,1)-1/2,np.random.rand(4,1)-1/2,np.random.rand(2,1)-1/2]\n",
    "list_activations = [\"ReLU\",\"ReLU\",\"sigmoide\"]\n",
    "\n",
    "Y_pred,caches = feed_forward(X,list_W,list_b,list_activations)\n",
    "\n",
    "Y = np.array([[0,1,1],[1,0,1]])\n",
    "\n",
    "list_dw, list_db = backward_propagation(Y_pred,Y,caches,list_activations)\n",
    "\n",
    "print(update_parameters(list_W,list_b,list_dw,list_db,0.01))"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "The expected result is:\n",
    "\n",
    "([array([[ 0.2854029 ,  0.35393053, -0.005905  ,  0.34642958, -0.42047817],\n",
    "       [ 0.00524609, -0.4347135 , -0.07187767, -0.40346908, -0.37284003],\n",
    "       [ 0.09674531, -0.273988  , -0.39305432, -0.27969379, -0.15017371],\n",
    "       [-0.0323369 , -0.29831753,  0.1403665 , -0.01701998,  0.00521764]]), array([[-1.13107349e-01,  2.93637454e-01,  8.00041789e-02,\n",
    "        -3.37701401e-01],\n",
    "       [ 2.00299434e-01,  4.64551080e-01,  8.36117022e-06,\n",
    "         3.89446821e-01],\n",
    "       [-1.58386347e-01,  6.71441276e-02, -7.24540367e-02,\n",
    "        -6.32527370e-02],\n",
    "       [ 2.76733818e-01,  3.56041735e-02,  4.53742227e-01,\n",
    "         4.41271238e-02]]), array([[-0.41790508, -0.1335902 ,  0.3508505 , -0.09354561],\n",
    "       [-0.47279763, -0.25277687, -0.43285563,  0.49399462]])], [array([[0.47046727],\n",
    "       [0.30025835],\n",
    "       [0.10181712],\n",
    "       [0.2647711 ]]), array([[-0.33077455],\n",
    "       [-0.20745432],\n",
    "       [ 0.02406688],\n",
    "       [-0.14343783]]), array([[-0.45151685],\n",
    "       [ 0.48355986]])])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parameters initialization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A last necessary step is missing: the parameters initialization. We will apply a random initialization of the parameters according to a central reduced normal distribution.\n",
    "\n",
    "**Exercise**: We give a neural network structure: a list that contains the number of neurons in each layer. The first element of the list is the input size, the next elements are the number of neurons in each layer.\n",
    "\n",
    "The following function must return an initial list of random matrices with weights W and biases b, drawned according to a normal distribution (with the correct dimensions). The list \"list_neurons\" contains the number of neurons in each layer and first element is the input dimension.\n",
    "\n",
    "As a reminder, the weight matrices have dimensions (number of neurons in layer l + 1, number of neurons in layer l). The biases vectors have dimensions (number of neurons in layer l). Note that it is not necessary to randomly initialize the biases vectors, we only fill them with zeros.\n",
    "\n",
    "**Hint**:\n",
    "- The functions np.random.randn and np.zeros are helpful"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def initialisation_parameters(list_neurons):\n",
    "    \n",
    "    N_couches = len(list_neurons)\n",
    "    \n",
    "    list_W = []\n",
    "    list_b = []\n",
    "    \n",
    "    for l in range(N_couches - 1):\n",
    "        \n",
    "        list_W.append(#TO BE FILLED)\n",
    "        list_b.append(#TO BE FILLED)\n",
    "    \n",
    "    return list_W, list_b"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Verification**: Run the following cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#DO NOT CHANGE\n",
    "\n",
    "list_neurons = [2,3,3,1]\n",
    "\n",
    "np.random.seed(seed = 2)\n",
    "\n",
    "print(initialisation_parameters(list_neurons))"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "The expected result is:\n",
    "\n",
    "([array([[-0.41675785, -0.05626683],\n",
    "       [-2.1361961 ,  1.64027081],\n",
    "       [-1.79343559, -0.84174737]]), array([[ 0.50288142, -1.24528809, -1.05795222],\n",
    "       [-0.90900761,  0.55145404,  2.29220801],\n",
    "       [ 0.04153939, -1.11792545,  0.53905832]]), array([[-0.5961597 , -0.0191305 ,  1.17500122]])], [array([[0.],\n",
    "       [0.],\n",
    "       [0.]]), array([[0.],\n",
    "       [0.],\n",
    "       [0.]]), array([[0.]])])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Let's start the training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We focus on the XOR problem (\"exclusive or\"). Let's take vectors with two dimensions. If both coordinates of the vector have the same sign, we will assign the vector to class 1, and 0 if the coordinates have opposite signs. As exercise, you can make a drawing of this situation. You can understand that a linear separator will not be able to separate the two classes. We will build a neural network to solve this problem. In the following cell, I build a database."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#NE PAS MODIFIER\n",
    "\n",
    "N_train = 200\n",
    "\n",
    "X_train = 2*np.random.rand(2,N_train) - 1\n",
    "\n",
    "Y_train = (np.sign(X_train[0,:]) == np.sign(X_train[1,:]))*1\n",
    "\n",
    "Y_train = np.reshape(Y_train,(1,N_train))\n",
    "\n",
    "N_test = 100\n",
    "\n",
    "X_test = 2*np.random.rand(2,N_test) - 1\n",
    "\n",
    "Y_test = (np.sign(X_test[0,:]) == np.sign(X_test[1,:]))*1\n",
    "\n",
    "Y_test = np.reshape(Y_test,(1,N_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise**: Initialize the network parameters according to the following architecture: [2,5,5,1]. The input dimension is 2 and the output dimension is 1 (we have only two classes, 0 or 1). Give also astivation functions (ReLU for the hidden layers and sigmoid for the last layer). Be careful: there are only two hidden layers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list_neurons = #TO BE FILLED\n",
    "\n",
    "list_activations = #TO BE FILLED\n",
    "\n",
    "list_W,list_b = #TO BE FILLED"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise**: Execute one thousand of learning epochs with a learning rate of 0.1. For each iteration, we display the value of the loss function on a test set and on the train set. Use the function that you have previously coded.\n",
    "\n",
    "**Hints**:\n",
    "- For each epoch, compute one forward propagation\n",
    "- Compute the loss function you obtain and display it (train set and test set)\n",
    "- Apply the backpropagation to compute the list of the gradients\n",
    "- Update the parameters according to these gradients\n",
    "- And we apply this procedure in a loop!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "N_epoques = 1000\n",
    "\n",
    "alpha = 0.1\n",
    "\n",
    "for i in range(N_epoques):\n",
    "    \n",
    "    Y_pred_train, caches = #TO BE FILLED\n",
    "    \n",
    "    Y_pred_test, _ = #TO BE FILLED\n",
    "    \n",
    "    loss_train = #TO BE FILLED\n",
    "    \n",
    "    loss_test = #TO BE FILLED\n",
    "    \n",
    "    print(\"Epoch\" + str(i))\n",
    "    print(\"Training loss: \" + str(loss_train))\n",
    "    print(\"Test loss: \" + str(loss_test))\n",
    "    \n",
    "    list_dW, list_db = #TO BE FILLED\n",
    "    \n",
    "    list_W,list_b = #TO BE FILLED"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Both loss functions should decrease if the learning is working. Find below the accuracy on the test set and on the train set: we use a decision threshold of 0.5; if the prediction is below 0.5, we associate the class 1, otherwise, we associate the class 0. We count the number of correct answers and we compute the ratio according to the number of examples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print(\"Accuracy on test set: \" + str(np.sum((Y_pred_test > 0.5) == Y_test)/N_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "print(\"Accuracy on train set: \" + str(np.sum((Y_pred_train > 0.5) == Y_train)/N_train))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can visualize your predictions on a 2D-square below!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grid = np.meshgrid(np.linspace(-1.,1.,50),np.linspace(-1,1,50))\n",
    "\n",
    "X_test_new = np.array([grid[0].flatten(),grid[1].flatten()])\n",
    "\n",
    "Y_pred_new, _ = feed_forward(X_test_new,list_W,list_b,list_activations)\n",
    "\n",
    "maps = plt.imshow((Y_pred_new.reshape(grid[0].shape[0],grid[0].shape[1])),extent = (-1,1,-1,1),cmap = \"hot\",origin = \"lower\")\n",
    "plt.scatter(X_test[0,:],X_test[1,:],color = \"blue\",marker = \"x\")\n",
    "plt.colorbar(maps, label = \"Network output\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
