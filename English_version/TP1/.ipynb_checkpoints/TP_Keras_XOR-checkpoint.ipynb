{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TP Programming with Keras - XOR problem"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this session, we build a neural network to solve the classification problem in the XOR situation, by using the Tensorflow/Keras libraries.\n",
    "\n",
    "Maintenant, nous allons produire un réseau de neurones pour effectuer une classification sur le cas XOR, en utilisant les bibliothèques tensorflow/keras.\n",
    "\n",
    "In this practice session, some cells must be filled according to the instructions. They are identified by the word **Exercise**. You will perform the **Verifications** yourselves in most cases, by watching if the algorithm correctly works and converges."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below we import the required libraries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow.keras as keras"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data definition"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We focus on the XOR problem (\"exclusive or\"). Let's take vectors with two dimensions. If both coordinates of the vector have the same sign, we will assign the vector to class 1, and 0 if the coordinates have opposite signs. As exercise, you can make a drawing of this situation. You can understand that a linear separator will not be able to separate the two classes. We will build a neural network to solve this problem. In the following cell, I build a database.\n",
    "\n",
    "Run the following cell to create the data. Do not hesitate to display a visualization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#DO NOT CHANGE\n",
    "\n",
    "N_train = 200\n",
    "\n",
    "X_train = 2*np.random.rand(N_train,2) - 1\n",
    "\n",
    "Y_train = (np.sign(X_train[:,0]) == np.sign(X_train[:,1]))*1\n",
    "\n",
    "Y_train = np.reshape(Y_train,(N_train,1))\n",
    "\n",
    "N_test = 100\n",
    "\n",
    "X_test = 2*np.random.rand(N_test,2) - 1\n",
    "\n",
    "Y_test = (np.sign(X_test[:,0]) == np.sign(X_test[:,1]))*1\n",
    "\n",
    "Y_test = np.reshape(Y_test,(N_test,1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Keras model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model creation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise**: Create a Keras model that you will call \"my_model\" with the following architecture: 3 Dense (fully-connected) layers, with 5, 5 and 1 neurons respectively. The first two layers must have a ReLU activation function and the last layer must have a sigmoid activation function.\n",
    "\n",
    "**Hints**:\n",
    "- Initialize the model with keras.Sequential\n",
    "- Add layers by using my_model.add(LAYER)\n",
    "- You will find the layers by calling keras.layers.Dense.\n",
    "- For the first layer, give the input shape by using the keyword \"input_shape\": in our case, the shape is (2,); keep the coma even if it seems useless (otherwise, it can create errors, depending on the Keras version)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_model = #TO BE FILLED\n",
    "\n",
    "#COMPLETE WITH THE STRUCTURE DEFINED IN THE INSTRUCTION"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise**: Display your architecture by calling my_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TO BE FILLED"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model compilation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise**: You must compile the model, by indicating the loss function to use, the different metrics to be monitored, and the optimizer you want to use (with the associated learning rate and optionally other hyperparameters depending on the optimizer).\n",
    "\n",
    "**Hints**:\n",
    "- First define the optimizer in the variable \"opt\". Use Adam optimizer (keras.optimizers.Adam), and use a learning rate (keyword lr) of 0.01.\n",
    "- Then use my_model.compile and add the following arguments:\n",
    "    - the optimizer with the keyword \"optimizer\" and give the variable opt that you previously created\n",
    "    - the loss function with the keyword \"loss\". We use the binary cross-entropy loss (string \"binary_crossentropy\")\n",
    "    - the metric with the keyword \"metrics\". You must provide a list that contains the relevant metrics to monitor. In our case, we use only one metric, \"binary_accuracy\", that corresponds to the rate of correct anwers (with a default decision threshold of 0.5). You can watch the evolution of the metric during the training phase."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "opt = #TO BE FILLED\n",
    "\n",
    "#COMPLETE WITH MY_MODEL.COMPILE ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise**: We must run the training!\n",
    "\n",
    "**Hints** :\n",
    "- Use model.fit and give as input:\n",
    "    - First the training input data X_train\n",
    "    - Then the expected output Y_train\n",
    "    - The number of epochs with the keyword \"epochs\". A hundred of epochs should be enough.\n",
    "- Store the learning information in a variable \"learning\" (learning = model.fit(...)), this is useful to recover information on the learning (loss or metric evolution for instance)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "learning = #TO BE FILLED"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Verification**: The loss function should decrease and the accuracy should increase."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise**: Complete the following code to draw the evolution of the loss and of the metric. You can find the loss function values by calling learning.history[\"loss\"] and the accuracy by calling learning.history[\"binary_accuracy\"]."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "loss_evolution = #TO BE FILLED\n",
    "acc_evolution = #TO BE FILLED\n",
    "\n",
    "plt.figure(figsize = (16,9))\n",
    "plt.subplot(121)\n",
    "plt.plot(loss_evolution)\n",
    "plt.xlabel(\"Epochs\")\n",
    "plt.ylabel(\"Loss function value\")\n",
    "plt.title(\"Loss function evolution\")\n",
    "\n",
    "plt.subplot(122)\n",
    "plt.plot(acc_evolution)\n",
    "plt.xlabel(\"Epochs\")\n",
    "plt.ylabel(\"Binary accuracy value\")\n",
    "plt.title(\"Binary accuracy evolution\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can think that we could have kept on training. The loss function can seem to be able to decrease. I can invite you to run the training again, with many more epochs (1000 or more), and you can watch the effect on the training accuracy and the testing accuracy."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Predictions with your model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise**: Now, we can perform predictions on the test set. You can just use my_model.predict and apply this function on the test set X_test. Store this prediction in a variable Y_pred_test."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y_pred_test = #A COMPLETER"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Find below the accuracy on the test set and on the train set: we use a decision threshold of 0.5; if the prediction is below 0.5, we associate the class 1, otherwise, we associate the class 0. We count the number of correct answers and we compute the ratio according to the number of examples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print(\"Accuracy on test set: \" + str(np.sum((Y_pred_test > 0.5) == Y_test)/N_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can visualize your predictions on a 2D-square below!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grid = np.meshgrid(np.linspace(-1.,1.,50),np.linspace(-1,1,50))\n",
    "\n",
    "X_test_new = np.array([grid[0].flatten(),grid[1].flatten()])\n",
    "\n",
    "Y_pred_new = my_model.predict(X_test_new.T)\n",
    "\n",
    "maps = plt.imshow((Y_pred_new.reshape(grid[0].shape[0],grid[0].shape[1])),extent = (-1,1,-1,1),cmap = \"hot\",origin = \"lower\")\n",
    "plt.scatter(X_test[:,0],X_test[:,1],color = \"blue\",marker = \"x\")\n",
    "plt.colorbar(maps, label = \"Network output\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
