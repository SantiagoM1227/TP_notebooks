{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TP Programming with Keras - Density Neural Network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will build a Density Neural Network in order to make uncertainty prediction in a regression case.\n",
    "\n",
    "In this practice session, some cells must be filled according to the instructions. They are identified by the word **Exercise**. You will perform the **Verifications** yourselves in most cases, by watching if the algorithm correctly works and converges."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below we import the required libraries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow.keras as keras\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data definition"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to easily visualize the data, we will use 1D data (as input and output): a cosinus with a gaussian noise. The following cell builds the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#DO NOT CHANGE\n",
    "N_train = 1000\n",
    "\n",
    "X_train = np.random.rand(N_train)*8 - 4\n",
    "\n",
    "sigma = np.abs(np.cos(X_train))*0.2\n",
    "\n",
    "Y_train = np.cos(X_train) + np.random.normal(0, sigma)\n",
    "\n",
    "N_test = 1000\n",
    "\n",
    "X_test = np.linspace(-8,8,N_test)\n",
    "\n",
    "Y_test = np.cos(X_test)\n",
    "\n",
    "X_train = np.reshape(X_train,(X_train.shape[0],1))\n",
    "\n",
    "Y_train = np.reshape(Y_train,(Y_train.shape[0],1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise**: Plot the training data by using a scatter plot."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#TO DO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creation of a density layer and the associated loss function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will assume that the output follows a probability density $p(y|x)$ from a normal distribution with mean $\\mu$ and standard deviation $\\sigma$. We aim to predict both parameters $\\mu$ and $\\sigma$. To do so, we create a custom layer, adapted to this problem.\n",
    "\n",
    "In the following cell, we create a new class DenseNormal which takes as argument the number $N$ of desired neurons and outputs $2N$ values: each neuron consists in one value of $\\mu$ and $\\sigma$.\n",
    "\n",
    "**Exercise**: $\\sigma$ must be strictly positive. To do so, apply the function tf.nn.softplus to logsigma. Moreover add a small $\\varepsilon$ to avoid divergence problems during the training (we avoid $\\sigma$ being too close to 0). 1e-6 should be enough."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DenseNormal(keras.layers.Layer):\n",
    "    def __init__(self, units):\n",
    "        super(DenseNormal, self).__init__()\n",
    "        self.units = int(units)\n",
    "        self.dense = keras.layers.Dense(2 * self.units)\n",
    "\n",
    "    def call(self, x):\n",
    "        output = self.dense(x)\n",
    "        mu, logsigma = tf.split(output, 2, axis=-1)\n",
    "        sigma = ##TO DO WITH tf.nn.softplus AND ADD A SMALL EPSILON\n",
    "        return tf.concat([mu, sigma], axis=-1)\n",
    "\n",
    "    def compute_output_shape(self, input_shape):\n",
    "        return (input_shape[0], 2 * self.units)\n",
    "\n",
    "    def get_config(self):\n",
    "        base_config = super(DenseNormal, self).get_config()\n",
    "        base_config['units'] = self.units\n",
    "        return base_config"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now created a custom loss-function. We recall the likelihood of the gaussian distribution:\n",
    "\n",
    "\\begin{equation}\n",
    "p(y|\\mu,\\theta) = \\frac{1}{\\sqrt{2\\pi}\\sigma} e^{-\\frac{(y-\\mu)^2}{2\\sigma^2}}\n",
    "\\end{equation}\n",
    "\n",
    "The associated log-likelihood is given by:\n",
    "\n",
    "\\begin{equation}\n",
    "\\log(p(y|\\mu,\\theta)) = -\\log(\\sqrt{2\\pi}\\sigma) - (\\frac{(y-\\mu)^2}{2\\sigma^2})\n",
    "\\end{equation}\n",
    "\n",
    "**Exercise**: Complete the following code by giving the full log-likelihood formula."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Gaussian_NLL(y, net_output, reduce=True):\n",
    "    mu, sigma = tf.split(net_output, 2, axis=-1)\n",
    "    ax = list(range(1, len(y.shape)))\n",
    "\n",
    "    logprob = #TO DO\n",
    "    loss = tf.reduce_mean(-logprob, axis=ax)\n",
    "    return tf.reduce_mean(loss) if reduce else loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Keras model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model creation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise**: Create a Keras model, with name \"my_model\". Use only Dense layer. The last layer will be a DenseNormal layer (previously created class), with only one neuron since the output is 1 dimensional."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TO DO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise**: Display your architecture by calling my_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TO DO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model compilation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise**: You must compile the model by defining an optimizer and a loss function. Use the loss function that you previously defined: you just have to enter the name of the loss function (without quoting marks). It is not necessary to use any metric."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TO DO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise**: Run the training as usual."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learning = #TO DO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Verification**: The loss function should decrease."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise**: Plot the evolution of the loss function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_evolution = learning.history[\"loss\"]\n",
    "\n",
    "\n",
    "plt.figure(figsize = (16,9))\n",
    "plt.plot(loss_evolution,label = \"Train set\")\n",
    "plt.xlabel(\"Epoques\")\n",
    "plt.ylabel(\"Valeur de la fonction de co√ªt\")\n",
    "plt.legend()\n",
    "plt.title(\"Loss function evolution\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercice**: Run a prediction on X_test."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y_pred_test = #TO DO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise**: Divide the prediction in two vectors mu and sigma."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mu = #TO DO\n",
    "sigma = #TO DO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following cell gives you a visualization of your prediction (mean) and the associated uncertainty. The grey areas correspond to uncertainty at 1, 2, 3 and 4 sigmas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "var = np.minimum(sigma, 1e3)\n",
    "    \n",
    "plt.figure(figsize=(5, 3), dpi=200)\n",
    "plt.title(\"Aleatoric uncertainty\")\n",
    "plt.scatter(X_train, Y_train, s=1., c='#463c3c', zorder=0, label=\"Train\")\n",
    "plt.plot(X_test, Y_test, 'r--', zorder=2, label=\"True\")\n",
    "plt.plot(X_test, mu, color='#007cab', zorder=3, label=\"Pred\")\n",
    "plt.plot([-4, -4], [-150, 150], 'k--', alpha=0.4, zorder=0)\n",
    "plt.plot([+4, +4], [-150, 150], 'k--', alpha=0.4, zorder=0)\n",
    "for k in np.linspace(0, 4, 4):\n",
    "    plt.fill_between(\n",
    "        X_test, (mu - k * var), (mu + k * var),\n",
    "        alpha=0.3,\n",
    "        edgecolor=None,\n",
    "        facecolor='#00aeef',\n",
    "        linewidth=0,\n",
    "        zorder=1,\n",
    "        label=\"Unc.\" if k == 0 else None)\n",
    "plt.gca().set_ylim(-2, 2)\n",
    "plt.gca().set_xlim(-7, 7)\n",
    "plt.legend(loc=\"upper left\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
