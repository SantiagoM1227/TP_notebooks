{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TP Programmation d'un réseau de neurones"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ce TP vise à construire votre propre réseau de neurones. Vous mettrez en place les fonctions de base qui permettent d'effectuer un calcul feed-forward d'un réseau de neurones ainsi que la backpropagation, pour ensuite effectuer un apprentissage sur le problème dit du XOR (ou exclusif). Ce TP permet de maîtriser les notions de base et comprendre les calculs effectués au sein d'un réseau de neurones lors d'une prédiction et lors de l'apprentissage. En revanche, par la suite, vous verrez que l'utilisation de bibliothèques dédiées comme Tensorflow/Keras est à privilégier : l'implémentation des codes est bien plus rapide et les algorithmes d'apprentissage sont optimisés pour tourner plus rapidement, exploiter les GPUs si votre machine en est équipée...\n",
    "\n",
    "Dans ce TP, des cellules seront laissées à trous, il faudra les compléter suivant les consignes. Elles seront identifiées par le mot **Exercice**. Certaines seront suivies de cellules vérifications qui vous permettront de vérifier si le résultat codé correspond bien au résultat attendu, elles seront précédées du mot **Vérification**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercice** : Importer numpy et matplotlib.pyplot et les nommer avec les mots-clés np et plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Vérification** : Exécuter la cellule suivante. Une droite doit apparaître."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "plt.plot(np.arange(5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Calcul forward pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dans cette première partie, nous allons créer les fonctions permettant d'effectuer le calcul forward pass d'un réseau de neurones."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fonctions d'activation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercice** : Commençons par coder les fonctions d'activation que nous allons utiliser. Nous allons principalement nous concentrer sur les fonctions d'activation ReLU (pour les couches intermédiaires du réseau) et sigmoïde (pour la sortie, problème de classification).\n",
    "\n",
    "En exercice, coder la fonction \"activation\" ci-dessous qui prend en argument un vecteur $Z$ une chaîne de caractère, soit \"ReLU\", soit \"sigmoide\" et qui renvoie le vecteur sur lequel est appliqué la fonction d'activation.\n",
    "\n",
    "On rappelle :\n",
    "- $ReLU(Z) = max(Z,0)$\n",
    "- $sigmoide(Z) = \\frac{1}{1 + e^{-Z}}$\n",
    "\n",
    "Pour la sigmoïde, je vous conseille de privilégier l'utilisation de la tangente hyperbolique pour effectuer le calcul, cela peut éviter des problèmes numérique dans le calcul de l'exponentiel. La sigmoïde peut effectivement s'exprimer à l'aide dans la fonction tanh."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def activation(Z,fonction):\n",
    "    \n",
    "    if fonction == \"ReLU\":\n",
    "        \n",
    "        resultat = #A COMPLETER\n",
    "    \n",
    "    if fonction == \"sigmoide\":\n",
    "        \n",
    "        resultat = #A COMPLETER\n",
    "    \n",
    "    return resultat"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Vérification** : Exécutez la cellule suivante"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#NE PAS MODIFIER\n",
    "\n",
    "Z = np.array([2.,10.,-5.,0.,-100.])\n",
    "\n",
    "print(activation(Z,\"ReLU\"))\n",
    "print(activation(Z,\"sigmoide\"))"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Le résultat doit être :\n",
    "\n",
    "[ 2. 10. -0.  0. -0.]\n",
    "[0.88079708 0.9999546  0.00669285 0.5        0.        ]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calcul d'une couche de neurones"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercice** : On considère ici une couche de neurones, représentée par un vecteur $A$ (de $n \\times N$ éléments représentant les $n$ neurones de la couche et $N$ représente le nombre d'exemples, car on peut avoir plusieurs exemples en même temps !). La couche suivante contiendra $m$ neurones, les poids reliant les deux couches seront représentés par une matrice $W$ de taille $m \\times n$, et les biais par un vecteur de taille $m$.\n",
    "\n",
    "Pour l'instant, on n'applique pas de fonction d'activation. On effectue le calcul linéaire : $Z = W.A + b$.\n",
    "\n",
    "Vous devrez aussi renvoyer une variable \"cache\" qui contiendra simplement le triplet $(A,W,b,Z)$ qui vous avez en entrée. Cette variable peut paraître inutile à ce stade, mais elle sera en fait très utile dans le cadre de la backpropagation : souvenez-vous, il faut utiliser l'état de l'ensemble des couches du réseau pour effectuer le calcul de backpropagation.\n",
    "\n",
    "**Hint** : Pour le calcul linéaire, la fonction np.dot devrait être utile."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def one_layer_linear(A,W,b):\n",
    "    \n",
    "    Z = #A COMPLETER\n",
    "    cache = #A COMPLETER\n",
    "    \n",
    "    return Z, cache"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Vérification** : Exécutez la cellule suivante :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#NE PAS MODIFIER\n",
    "\n",
    "np.random.seed(seed = 1)\n",
    "\n",
    "A = np.random.rand(3,2)\n",
    "W = np.random.rand(4,3)\n",
    "b = np.random.rand(4,1)\n",
    "\n",
    "Z, cache = one_layer_linear(A,W,b)\n",
    "\n",
    "print(Z)\n",
    "print(cache == (A,W,b,Z))"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Le résultat doit être : \n",
    "\n",
    "[[0.27632903 0.41566595]\n",
    " [0.52340787 0.77623274]\n",
    " [0.89012538 1.21602897]\n",
    " [1.32990003 1.62896921]]\n",
    "True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercice** : Cette fois, on fait le calcul en entier, avec en plus la fonction d'activation. Notons $g$ la fonction d'activation, $A_{prev}$ le vecteur représentant la couche précédente, $A_{new}$ le résultat de la nouvelle couche, on a donc : $A_{new} = g(W.A_{prev} + b)$. La variable \"fonction\" contient une chaîne de caractère qui définit la fonction (soit \"ReLU\", soit \"sigmoide\" dans notre cas).\n",
    "\n",
    "La variable cache devra contenir les mêmes quantités que précédemment.\n",
    "\n",
    "**Hint** : Réutilisez les fonctions \"activation\" et \"one_layer_linear\" que vous avez codées précédemment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def one_layer_activation(A_prev,W,b,fonction):\n",
    "    \n",
    "    Z, cache = #A COMPLETER\n",
    "    A_new = #A COMPLETER\n",
    "        \n",
    "    return A_new, cache"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Vérification** : Exécutez la cellule ci-dessous :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#NE PAS MODIFIER\n",
    "\n",
    "np.random.seed(seed = 1)\n",
    "\n",
    "A_prev = np.random.rand(3,2)\n",
    "W = np.random.rand(4,3)\n",
    "b = np.random.rand(4,1)\n",
    "\n",
    "Z, cache_old = one_layer_linear(A_prev,W,b)\n",
    "\n",
    "A_new, cache_new = one_layer_activation(A_prev,W,b,\"sigmoide\")\n",
    "print(A_new)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Le résultat doit être :\n",
    "\n",
    " [[0.56864601 0.60244568]\n",
    " [0.62794429 0.68486762]\n",
    " [0.70891605 0.77136397]\n",
    " [0.7908241  0.83602838]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calcul du réseau de neurones entier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercice** : Supposons que nous avons une liste de paramètres \"List_W\" et \"List_b\" qui contiennent l'ensemble des paramètres du réseau (poids et biais) pour chaque couche, ainsi qu'une entrée $X$ de taille $n_X \\times N$, où $n_X$ est la dimension des inputs du réseau de neurones et $N$ est le nombre d'exemples.\n",
    "\n",
    "\"List_W\" et \"List_b\" ont la même taille (le nombre de couches du réseau). On se donne aussi \"List_activ\" qui contient les fonctions d'activation à appliquer.\n",
    "\n",
    "Codez la fonction ci-dessous qui applique le calcul feed_forward en entier, sur l'ensemble des couches.\n",
    "\n",
    "La variable \"caches\" sera une liste qui contiendra l'ensemble des caches calculés à chaque couche du réseau. La variable Y_pred contiendra la sortie du réseau."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Hints** :\n",
    "- Cette fonction sera composée d'une boucle principale qui parcourera l'ensemble des couches du réseau.\n",
    "- Réutilisez bien sûr la fonction \"one_layer_activation\" pour chaque couche.\n",
    "- Pour connaître le nombre de couches, regardez la taille de l'une des listes de paramètres en entrée (np.size sera utile).\n",
    "- Ajouter des éléments elem dans une liste l se fait en utilisant l.append(elem)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def feed_forward(X,list_W,list_b,list_activations):\n",
    "    \n",
    "    A = #A COMPLETER\n",
    "    N_couches = #A COMPLETER\n",
    "    caches = []\n",
    "    \n",
    "    for i in range(N_couches):\n",
    "    \n",
    "        #A COMPLETER\n",
    "    \n",
    "    Y_pred = A\n",
    "    \n",
    "    return Y_pred,caches"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Vérification** : Exécutez la cellule ci-dessous"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#NE PAS MODIFIER\n",
    "\n",
    "np.random.seed(seed = 2)\n",
    "\n",
    "X = np.random.rand(5,3)\n",
    "list_W = [np.random.rand(4,5)-1/2,np.random.rand(4,4)-1/2,np.random.rand(2,4)-1/2]\n",
    "list_b = [np.random.rand(4,1)-1/2,np.random.rand(4,1)-1/2,np.random.rand(2,1)-1/2]\n",
    "list_activations = [\"ReLU\",\"ReLU\",\"sigmoide\"]\n",
    "\n",
    "Y_pred,caches = feed_forward(X,list_W,list_b,list_activations)\n",
    "\n",
    "print(Y_pred)\n",
    "print(len(caches) == 3)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Le résultat doit être : \n",
    "\n",
    "[[0.3863283  0.38738737 0.38502977]\n",
    " [0.62732414 0.62279049 0.62796192]]\n",
    "True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loss function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nous allons implémenter maintenant la loss function que nous allons utiliser. Ici, ce sera la binary_cross_entropy. Nous allons faire de la classification multi-classes non exclusives avec $K$ classes.\n",
    "\n",
    "Nous redonnons la formule ci-dessous, pour un vecteur de prédictions $\\hat{Y}$ composé de $N$ exemples $\\hat{y_ik}$ qui doit être comparé à un vecteur de vraies valeurs $Y$ composé d'exemples $y_{ik}$ (l'indice $i$ représente l'exemple $i$ et l'indice $k$ représente la classe $k$):\n",
    "\n",
    "\\begin{equation}\n",
    "L(Y,\\hat{Y}) = - \\frac{1}{N} \\sum_{i=1}^{N} \\sum_{k=1}^{K} y_{ik} \\log(\\hat{y_{ik}}) + (1-y_{ik})\\log(1-\\hat{y_{ik}})\n",
    "\\end{equation}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercice** : Codez la loss-function ci-dessous, où la variable Y_pred représente les prédictions.\n",
    "\n",
    "**Hints** :\n",
    "- Déterminez le nombre d'exemple à partir de la taille de Y (seconde dimension), la fonction shape sera utile\n",
    "- Les fonctions np.sum et np.log seront utiles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss_function(Y,Y_pred):\n",
    "    \n",
    "    N = #A COMPLETER\n",
    "    \n",
    "    loss = #A COMPLETER\n",
    "    \n",
    "    return loss\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Vérification** : Exécutez la cellule ci-dessous"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#NE PAS MODIFIER\n",
    "\n",
    "np.random.seed(seed = 3)\n",
    "\n",
    "Y = np.array([[1,0,1],[0,1,1]])\n",
    "Y_pred = np.random.rand(2,3)\n",
    "\n",
    "print(loss_function(Y,Y_pred))"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Le résultat doit être : 1.3334702695881486"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Backward propagation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dans cette partie, nous allons programmer la backpropagation de l'erreur, qui permettra de calculer les gradients et ainsi mettre à jour les paramètres du réseau de neurones."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Backward linéraire"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Considérons une couche $l$. On rappelle le calcul feed_forward : $Z^{[l]} = W^{[l]}.A^{[l-1]} + b^{[l]}$.\n",
    "\n",
    "Supposons déjà connue la dérivée de la fonction de coût par rapport à $Z$ (de manière récursive) $\\frac{\\partial L}{\\partial Z^{[l]}}$.\n",
    "\n",
    "Nous allons calculer $\\frac{\\partial L}{\\partial W^{[l]}}$, $\\frac{\\partial L}{\\partial b^{[l]}}$ et $\\frac{\\partial L}{\\partial A^{[l-1]}}$. Les deux premières permettent d'obtenir le gradient des paramètres de la couche $l$. La dernière dérivée sera utilisée pour la récursivité de la backpropagation.\n",
    "\n",
    "On rappelle les formules (correspondant aux moyennes sur l'ensemble des exemples, d'où la présence des transposées et de la somme pour b) :\n",
    "\n",
    "\\begin{equation}\n",
    "\\frac{\\partial L}{\\partial W^{[l]}} = \\frac{\\partial L}{\\partial Z^{[l]}} \\frac{\\partial Z^{[l]}}{\\partial W^{[l]}} = \\frac{\\partial L}{\\partial Z^{[l]}} A^{[l-1]T}\n",
    "\\end{equation}\n",
    "\n",
    "\\begin{equation}\n",
    "\\frac{\\partial L}{\\partial b^{[l]}} = \\sum_{i = 1}^{N} (\\frac{\\partial L}{\\partial Z^{[l]}})_i \n",
    "\\end{equation}\n",
    "\n",
    "\\begin{equation}\n",
    "\\frac{\\partial L}{\\partial A^{[l-1]}} = \\frac{\\partial L}{\\partial Z^{[l]}} \\frac{\\partial Z^{[l]}}{\\partial A^{[l-1]}} = W^{[l]T}\\frac{\\partial L}{\\partial Z^{[l]}} \n",
    "\\end{equation}\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercice** : Codez ci-dessous ces trois formules dans la fonction \"linear_backward\". En entrée, on prend dZ, correspondant à la dérivée de Z supposée connue, et le cache qui contient (A_prev,W,b,Z), d'où l'utilité de ce cache que nous avions défini dans les fonctions feed_forward.\n",
    "\n",
    "**Hint** : Pour la somme, bien faire attention à la dimension sur laquelle on l'applique (dimension correspondant aux exemples), à utiliser avec le mot-clé \"axis = ...\". Utilisez bien aussi le mot clé \"keepdims = True\" pour pouvoir maintenir les opérations valides au niveau de la dimension."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def linear_backward(dZ,cache):\n",
    "    \n",
    "    A_prev,W,b,Z = cache\n",
    "    \n",
    "    dW = #A COMPLETER\n",
    "    db = #A COMPLETER\n",
    "    dA_prev = #A COMPLETER\n",
    "    \n",
    "    return dA_prev,dW,db"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Vérification** : Exécutez la cellule ci-dessous"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#NE PAS MODIFIER\n",
    "\n",
    "np.random.seed(seed = 3)\n",
    "\n",
    "dZ = np.random.rand(3,2)\n",
    "\n",
    "A_pred = np.random.rand(4,2)\n",
    "W = np.random.rand(3,4)\n",
    "b = np.random.rand(3,1)\n",
    "Z = np.random.rand(3,2)\n",
    "\n",
    "cache =  (A_pred,W,b,Z)\n",
    "\n",
    "print(linear_backward(dZ,cache))"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Le résultat doit être :\n",
    "(array([[0.84119935, 1.00609737],\n",
    "       [0.58627548, 0.77106284],\n",
    "       [0.58203064, 0.64998031],\n",
    "       [1.20623246, 1.44921602]]), array([[0.21593072, 0.34050656, 0.33996121, 0.55475734],\n",
    "       [0.14239875, 0.24014989, 0.24205415, 0.33109807],\n",
    "       [0.29789138, 0.4410523 , 0.43613433, 0.82925743]]), array([[1.25894573],\n",
    "       [0.80173234],\n",
    "       [1.78924004]]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ajoutons l'activation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercice** : Dans un premier temps, nous allons calculer les dérivées des fonctions d'activation que nous avons utilisées.\n",
    "\n",
    "Nous rappelons que :\n",
    "- Pour $g = ReLU$, $g'(x) = 0$ si $x < 0$ et $g'(x) = 1$ si $x \\ge 0$.\n",
    "- Pour $g = sigmoide$, $g'(x) = g(x)(1-g(x))$\n",
    "\n",
    "**Hints** : \n",
    "- Ne mettez pas de condition dans ReLU de type if, then ! Utilisez directement l'écriture booléenne $Z > 0$.\n",
    "- Pour la sigmoïde, réutilisez la fonction activation que vous aviez codée au tout début"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def derivate_activation(Z,activ):\n",
    "    \n",
    "    if activ == \"ReLU\":\n",
    "        \n",
    "        dg = #A COMPLETER\n",
    "        \n",
    "    if activ == \"sigmoide\":\n",
    "        \n",
    "        dg = #A COMPLETER\n",
    "    \n",
    "    return dg"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Vérification** : Exécutez la cellule suivante"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#NE PAS MODIFIER\n",
    "\n",
    "np.random.seed(seed = 4)\n",
    "\n",
    "Z = np.random.rand(4,2) - 1/2\n",
    "\n",
    "print(derivate_activation(Z,\"ReLU\"))\n",
    "print(derivate_activation(Z,\"sigmoide\"))"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Le résultat doit être :\n",
    "[[1. 1.]\n",
    " [1. 1.]\n",
    " [1. 0.]\n",
    " [1. 0.]]\n",
    "[[0.23684838 0.24986062]\n",
    " [0.23653961 0.24713792]\n",
    " [0.24757229 0.24502909]\n",
    " [0.23634193 0.23536043]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercice** : On ajoute maintenant la dérivée de la fonction d'activation afin de calculer $\\frac{\\partial L}{\\partial Z^{[l]}}$ à partir de $\\frac{\\partial L}{\\partial A^{[l]}}$ (supposée connue par récursivité... vu qu'on la calcule avec la fonction précédente).\n",
    "\n",
    "On rappelle la formule :\n",
    "\n",
    "\\begin{equation}\n",
    "A^{[l]} = g(Z^{[l]})\n",
    "\\end{equation}\n",
    "\n",
    "Ce qui donne donc :\n",
    "\n",
    "\\begin{equation}\n",
    "\\frac{\\partial L}{\\partial Z^{[l]}} = \\frac{\\partial L}{\\partial A^{[l]}} \\frac{\\partial A^{[l]}}{\\partial Z^{[l]}} = \\frac{\\partial L}{\\partial A^{[l]}} g'(Z^{[l]})\n",
    "\\end{equation}\n",
    "\n",
    "La fonction d'activation est donnée par activ.\n",
    "\n",
    "**Hint** :\n",
    "- La valeur de $Z^{[l]}$ sera utile, on rappelle qu'elle est justement dans le cache qui contient A_prev,W,b,Z\n",
    "- Réutilisez les fonctions précédentes \"linear_backward\" pour retrouver les valeurs dA_prev, dW et db à partir du dZ que vous aurez calculé et derivate_activation pour avoir les dérivées des fonctions d'activation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def activation_backward(dA,cache,activ):\n",
    "    \n",
    "    A_prev,W,b,Z = cache\n",
    "    \n",
    "    dZ = #A COMPLETER\n",
    "    \n",
    "    dA_prev,dW,db = #A COMPLETER\n",
    "    \n",
    "    return dA_prev,dW,db"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Vérification** : Exécutez la cellule ci-dessous"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(seed = 3)\n",
    "\n",
    "dA = np.random.rand(3,2)\n",
    "\n",
    "A_pred = np.random.rand(4,2)\n",
    "W = np.random.rand(3,4)\n",
    "b = np.random.rand(3,1)\n",
    "Z = np.random.rand(3,2)\n",
    "\n",
    "cache =  (A_pred,W,b,Z)\n",
    "\n",
    "print(activation_backward(dA,cache,\"sigmoide\"))"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Le résultat devrait être :\n",
    "(array([[0.17487705, 0.217628  ],\n",
    "       [0.12229471, 0.16062103],\n",
    "       [0.12332169, 0.1508871 ],\n",
    "       [0.25380512, 0.32009158]]), array([[0.04316761, 0.06778688, 0.06763646, 0.11153914],\n",
    "       [0.02990346, 0.04956865, 0.04984338, 0.07145092],\n",
    "       [0.06838174, 0.10498491, 0.10439884, 0.18202664]]), array([[0.25220214],\n",
    "       [0.16993783],\n",
    "       [0.40389105]]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Et on fait remonter le long du réseau"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On part maintenant de la fin du réseau et on fait remonter le long des couches.\n",
    "\n",
    "**Exercice** : Appliquez la backpropagation ci-dessous. \n",
    "\n",
    "**Hints** :\n",
    "- Il faut dans un premier initialiser en calculant la dérivée de la loss function pour la dernière couche :\n",
    "\\begin{equation}\n",
    "\\frac{\\partial L}{\\partial A^{[N_{couches}]}} = \\frac{\\partial L}{\\partial \\hat{Y}^{[l]}} = - \\frac{1}{N} (\\frac{Y}{\\hat{Y}} - \\frac{1 - Y}{1 - \\hat{Y}}) \n",
    "\\end{equation}\n",
    "\n",
    "- On fait ensuite une boucle en remontant le sens des couches (notez l'utilisation de reversed(range(Ncouches))\n",
    "\n",
    "- Utilisez les caches qui sont stockés sous forme de liste\n",
    "\n",
    "- Stockez enfin les gradients des paramètres dans deux listes list_dW et list_db : attention de les insérez à chaque fois au début de la liste itérativement (et non à la fin) car on remonte les couches dans la backpropagation ! La fonction l.insert(0,var) sera utile pour cela, où l est la liste, 0 est la position (au début de la liste) et var est la variable à insérer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def backward_propagation(Y_pred,Y,caches,activations):\n",
    "    \n",
    "    list_dW = []\n",
    "    list_db = []\n",
    "    \n",
    "    N_couches = #A COMPLETER\n",
    "    N = #A COMPLTER (nombre d'exemples)\n",
    "    Y = Y.reshape(np.shape(Y_pred))\n",
    "    \n",
    "    dA_cur = #A COMPLETER (initialisation : dérivée de la dernière couche)\n",
    "    \n",
    "    for l in reversed(range(N_couches)):\n",
    "\n",
    "        dA_prev,dW,db = #A COMPLETER\n",
    "        \n",
    "        #A COMPLETER : insertion dans les listes list_dW, list_db\n",
    "        \n",
    "        dA_cur = #A COMPLETER\n",
    "    \n",
    "    return list_dW, list_db      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#NE PAS MODIFIER\n",
    "\n",
    "np.random.seed(seed = 2)\n",
    "\n",
    "X = np.random.rand(5,3)\n",
    "list_W = [np.random.rand(4,5)-1/2,np.random.rand(4,4)-1/2,np.random.rand(2,4)-1/2]\n",
    "list_b = [np.random.rand(4,1)-1/2,np.random.rand(4,1)-1/2,np.random.rand(2,1)-1/2]\n",
    "list_activations = [\"ReLU\",\"ReLU\",\"sigmoide\"]\n",
    "\n",
    "Y_pred,caches = feed_forward(X,list_W,list_b,list_activations)\n",
    "\n",
    "Y = np.array([[0,1,1],[1,0,1]])\n",
    "\n",
    "print(backward_propagation(Y_pred,Y,caches,list_activations))"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "([array([[-0.00677534,  0.00447579,  0.01418342,  0.01319047,  0.01236515],\n",
    "       [ 0.        ,  0.        ,  0.        ,  0.        ,  0.        ],\n",
    "       [ 0.        ,  0.        ,  0.        ,  0.        ,  0.        ],\n",
    "       [ 0.01243833,  0.00607596,  0.0040221 ,  0.00898123,  0.00190816]]), array([[ 0.        ,  0.        ,  0.        ,  0.        ],\n",
    "       [ 0.04529126,  0.        ,  0.        ,  0.00732433],\n",
    "       [ 0.        ,  0.        ,  0.        ,  0.        ],\n",
    "       [-0.01746327,  0.        ,  0.        ,  0.00810363]]), array([[ 0.        , -0.00673947,  0.        , -0.01793444],\n",
    "       [ 0.        , -0.00458954,  0.        , -0.01426107]])], [array([[0.01130477],\n",
    "       [0.        ],\n",
    "       [0.        ],\n",
    "       [0.01887614]]), array([[0.        ],\n",
    "       [0.04775503],\n",
    "       [0.        ],\n",
    "       [0.00621147]]), array([[-0.28041819],\n",
    "       [-0.04064115]])])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Mise à jour des paramètres"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercice** : Une fois les gradients des paramètres calculés, il suffit de mettre à jour les anciens paramètres et les remplacer par les nouveaux. Codez la fonction qui prend la liste des anciens paramètres (list_w,list_b), la liste des gradients (list_dw,list_db) et le taux d'apprentissage alpha, pour renvoyer la liste des nouveaux paramètres suivant la formule de mise à jour du gradient :\n",
    "\n",
    "\\begin{equation}\n",
    "W^{[l]} := W^{[l]} - \\alpha \\frac{\\partial L}{\\partial W^{[l]}}\n",
    "\\end{equation}\n",
    "\n",
    "\\begin{equation}\n",
    "b^{[l]} := b^{[l]} - \\alpha \\frac{\\partial L}{\\partial b^{[l]}}\n",
    "\\end{equation}\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_parameters(list_w,list_b,list_dw,list_db,alpha):\n",
    "    \n",
    "    N_couches = #A COMPLETER\n",
    "    \n",
    "    for l in range(N_couches):\n",
    "        \n",
    "        list_w[l] = #A COMPLETER\n",
    "        list_b[l] = #A COMPLETER\n",
    "    \n",
    "    return list_w,list_b"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Vérification** : Exécutez la cellule ci-dessous"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#NE PAS MODIFIER\n",
    "\n",
    "np.random.seed(seed = 2)\n",
    "\n",
    "X = np.random.rand(5,3)\n",
    "list_W = [np.random.rand(4,5)-1/2,np.random.rand(4,4)-1/2,np.random.rand(2,4)-1/2]\n",
    "list_b = [np.random.rand(4,1)-1/2,np.random.rand(4,1)-1/2,np.random.rand(2,1)-1/2]\n",
    "list_activations = [\"ReLU\",\"ReLU\",\"sigmoide\"]\n",
    "\n",
    "Y_pred,caches = feed_forward(X,list_W,list_b,list_activations)\n",
    "\n",
    "Y = np.array([[0,1,1],[1,0,1]])\n",
    "\n",
    "list_dw, list_db = backward_propagation(Y_pred,Y,caches,list_activations)\n",
    "\n",
    "print(update_parameters(list_W,list_b,list_dw,list_db,0.01))"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Le résultat doit être :\n",
    "\n",
    "([array([[ 0.2854029 ,  0.35393053, -0.005905  ,  0.34642958, -0.42047817],\n",
    "       [ 0.00524609, -0.4347135 , -0.07187767, -0.40346908, -0.37284003],\n",
    "       [ 0.09674531, -0.273988  , -0.39305432, -0.27969379, -0.15017371],\n",
    "       [-0.0323369 , -0.29831753,  0.1403665 , -0.01701998,  0.00521764]]), array([[-1.13107349e-01,  2.93637454e-01,  8.00041789e-02,\n",
    "        -3.37701401e-01],\n",
    "       [ 2.00299434e-01,  4.64551080e-01,  8.36117022e-06,\n",
    "         3.89446821e-01],\n",
    "       [-1.58386347e-01,  6.71441276e-02, -7.24540367e-02,\n",
    "        -6.32527370e-02],\n",
    "       [ 2.76733818e-01,  3.56041735e-02,  4.53742227e-01,\n",
    "         4.41271238e-02]]), array([[-0.41790508, -0.1335902 ,  0.3508505 , -0.09354561],\n",
    "       [-0.47279763, -0.25277687, -0.43285563,  0.49399462]])], [array([[0.47046727],\n",
    "       [0.30025835],\n",
    "       [0.10181712],\n",
    "       [0.2647711 ]]), array([[-0.33077455],\n",
    "       [-0.20745432],\n",
    "       [ 0.02406688],\n",
    "       [-0.14343783]]), array([[-0.45151685],\n",
    "       [ 0.48355986]])])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initialisation des paramètres"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Il nous manque une étape essentielle : l'initialisation des paramètres. Nous allons faire une initialisation aléatoire des paramètres, suivant une loi normale centrée réduite.\n",
    "\n",
    "**Exercice** : On se donne une structure de réseau de neurones : une liste contenant le nombre de neurones de chaque couche. Le premier élément de la liste est la taille de la donnée d'entrée, les éléments suivant sont le nombre de neurones de chaque couche.\n",
    "\n",
    "Votre exercice est de fournir une liste initiale de matrices aléatoires de poids W et b, tirés suivant une loi normale centrée réduite, avec les bonnes dimensions. La liste list_neurons contient le nombre de neurones de chaque couche et le premier élément est la dimension des données d'entrée.\n",
    "\n",
    "Nous rappelons que les matrices des poids sont de forme (nombre de neurones couche l + 1, nombre de neurones couche l). Les vecteurs de biais sont de formes (nombre de neurones couche l). Notons qu'il n'est pas nécessaire d'initialiser aléatoirement les vecteurs de biais. Nous allons ici ne mettre que des zéros.\n",
    "\n",
    "**Hint** :\n",
    "- Les fonctions np.random.randn et np.zeros seront utiles\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def initialisation_parameters(list_neurons):\n",
    "    \n",
    "    N_couches = len(list_neurons)\n",
    "    \n",
    "    list_W = []\n",
    "    list_b = []\n",
    "    \n",
    "    for l in range(N_couches - 1):\n",
    "        \n",
    "        list_W.append(#A COMPLETER)\n",
    "        list_b.append(#A COMPLETER)\n",
    "    \n",
    "    return list_W, list_b"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Vérification** : Exécutez la cellule suivante"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#NE PAS MODIFIER\n",
    "\n",
    "list_neurons = [2,3,3,1]\n",
    "\n",
    "np.random.seed(seed = 2)\n",
    "\n",
    "print(initialisation_parameters(list_neurons))"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Le résultat doit être :\n",
    "\n",
    "([array([[-0.41675785, -0.05626683],\n",
    "       [-2.1361961 ,  1.64027081],\n",
    "       [-1.79343559, -0.84174737]]), array([[ 0.50288142, -1.24528809, -1.05795222],\n",
    "       [-0.90900761,  0.55145404,  2.29220801],\n",
    "       [ 0.04153939, -1.11792545,  0.53905832]]), array([[-0.5961597 , -0.0191305 ,  1.17500122]])], [array([[0.],\n",
    "       [0.],\n",
    "       [0.]]), array([[0.],\n",
    "       [0.],\n",
    "       [0.]]), array([[0.]])])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Et c'est parti pour l'entraînement"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nous allons nous intéresser au problème du XOR (ou exclusif). Prenons des vecteurs à deux dimensions. Si les deux coordonnées ont le même signe, on dira que le vecteur appartient à la classe 1, et 0 si les deux coordonnées sont de signes opposés. En exercice, vous pouvez faire un dessin représentant la situation. On comprend ici qu'un séparateur linéaire ne permettra pas de séparer les deux classes. Nous allons faire un réseau de neurones pour cela. Ci-dessous, je vous construis une base de données."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#NE PAS MODIFIER\n",
    "\n",
    "N_train = 200\n",
    "\n",
    "X_train = 2*np.random.rand(2,N_train) - 1\n",
    "\n",
    "Y_train = (np.sign(X_train[0,:]) == np.sign(X_train[1,:]))*1\n",
    "\n",
    "Y_train = np.reshape(Y_train,(1,N_train))\n",
    "\n",
    "N_test = 100\n",
    "\n",
    "X_test = 2*np.random.rand(2,N_test) - 1\n",
    "\n",
    "Y_test = (np.sign(X_test[0,:]) == np.sign(X_test[1,:]))*1\n",
    "\n",
    "Y_test = np.reshape(Y_test,(1,N_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercice** : Initialiser les paramètres d'un réseau d'architecture suivante : [2,5,5,1]. L'entrée est en effet de dimension 2 et la sortie de dimension 1. Donnez aussi les fonctions d'activation pour l'ensemble des couches intermédiaires et sigmoïde pour la dernière sortie (attention, il n'y a que 3 couches)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list_neurons = #A COMPLETER\n",
    "\n",
    "list_activations = #A COMPLETER\n",
    "\n",
    "list_W,list_b = #A COMPLETER"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercice** : Effectuez un millier d'époques d'apprentissage avec un taux d'apprentissage de 0.1. A chaque itération, affichez la valeur de la loss function sur la base de test et la base d'apprentissage. Utilisez les fonctions que vous avez définies ci-dessus.\n",
    "\n",
    "**Hint** :\n",
    "- A chaque époque, effectuez une propagation forward pass\n",
    "- Calculez la loss function que vous obtenez et affichez-la (base d'entraînement et base de test)\n",
    "- Effectuez la back-propagation pour calculer les listes de gradients\n",
    "- Mettez à jour les paramètres\n",
    "- Et on boucle !"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "N_epoques = 1000\n",
    "\n",
    "alpha = 0.1\n",
    "\n",
    "for i in range(N_epoques):\n",
    "    \n",
    "    Y_pred_train, caches = #A COMPLETER\n",
    "    \n",
    "    Y_pred_test, _ = #A COMPLETER\n",
    "    \n",
    "    loss_train = #A COMPLETER\n",
    "    \n",
    "    loss_test = #A COMPLETER\n",
    "    \n",
    "    print(\"Epoque\" + str(i))\n",
    "    print(\"Training loss: \" + str(loss_train))\n",
    "    print(\"Test loss: \" + str(loss_test))\n",
    "    \n",
    "    list_dW, list_db = #A COMPLETER\n",
    "    \n",
    "    list_W,list_b = #A COMPLETER"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Les loss functions devraient décroître si votre apprentissage se passent bien. Ci dessous, vous trouverez la précision sur le jeu de test, ainsi que sur le jeu d'entraînement : on se fixe un seuil de 0.5, si la prédiction est supérieure à 0.5, on l'associe à la classe 1, et à la classe 0 sinon. On compte le nombre de fois où la classe prédite est égale à la classe attendue et on regarde par rapport au nombre d'exemples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print(\"Accuracy sur le jeu de test : \" + str(np.sum((Y_pred_test > 0.5) == Y_test)/N_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "print(\"Accuracy sur le jeu d'entraînement : \" + str(np.sum((Y_pred_train > 0.5) == Y_train)/N_train))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ci-dessous pour visualiser vos prédictions sur l'ensemble du carré unité en deux dimensions !"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grid = np.meshgrid(np.linspace(-1.,1.,50),np.linspace(-1,1,50))\n",
    "\n",
    "X_test_new = np.array([grid[0].flatten(),grid[1].flatten()])\n",
    "\n",
    "Y_pred_new, _ = feed_forward(X_test_new,list_W,list_b,list_activations)\n",
    "\n",
    "maps = plt.imshow((Y_pred_new.reshape(grid[0].shape[0],grid[0].shape[1])),extent = (-1,1,-1,1),cmap = \"hot\",origin = \"lower\")\n",
    "plt.scatter(X_test[0,:],X_test[1,:],color = \"blue\",marker = \"x\")\n",
    "plt.colorbar(maps, label = \"Sortie du réseau de neurones\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
