{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Mg6lOuugLYbb"
      },
      "source": [
        "# TP Programming with Keras - MNIST problem, autoencoder"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VdKGGA28LYbg"
      },
      "source": [
        "We will build an autoencoder neural network on the MNIST database. The objective of an autoencoder is to be able to reduce the data dimension in a non-linear way, into a \"latent space\". It is a kind of non-linear version of a Principal Component Analysis. The first part is the encoder and proceed to this dimension reduction. The second part of the network, the decoder, recovers the data from the latent space.\n",
        "\n",
        "In this practice session, some cells must be filled according to the instructions. They are identified by the word **Exercise**. You will perform the **Verifications** yourselves in most cases, by watching if the algorithm correctly works and converges."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tHwLDU0VLYbg"
      },
      "source": [
        "Below we import the required libraries."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "c9qnNzPDLYbh"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import tensorflow.keras as keras\n",
        "import tensorflow as tf"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2Xxy43JmLYbi"
      },
      "source": [
        "## Data definition"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eHKfTbGkLYbi"
      },
      "source": [
        "The following cell loads the MNIST data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "swCYNKB5LYbj",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "bb317c0a-9392-4bb5-d2ac-01842ed34751"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/mnist.npz\n",
            "\u001b[1m11490434/11490434\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 0us/step\n"
          ]
        }
      ],
      "source": [
        "#DO NOT CHANGE\n",
        "\n",
        "(X_train, Y_train), (X_test, Y_test) = keras.datasets.mnist.load_data()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "i3u6fhHULYbk"
      },
      "source": [
        "**Exercise**: Apply data normalization (division by 255) and change the output data into categorial vectors (one hot encoding with keras.utils.categorical)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "EW0XjTH1LYbk"
      },
      "outputs": [],
      "source": [
        "X_train = X_train/255\n",
        "X_test = X_test/255"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GnkwLyXTLYbl"
      },
      "source": [
        "**Exercise**: We will use 2D convolutional layer in this autoencoder. Adapt the dimensions of X_train in order to use this type of layer (reminder: you need 3 dimensions)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "5N1VctcNLYbm"
      },
      "outputs": [],
      "source": [
        "X_train = np.expand_dims(X_train,axis = 3)\n",
        "X_test = np.expand_dims(X_test,axis = 3)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "13KyHuUWLYbm"
      },
      "source": [
        "## Keras model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rDxyw0yKLYbn"
      },
      "source": [
        "### Autoencoder creation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "i_hgoiIZLYbn"
      },
      "source": [
        "**Exercise**: Create a model named \"my_model\".\n",
        "\n",
        "**Specific instructions**:\n",
        "\n",
        "- Use the format below: this is no longer a sequential format, because a specific option is needed to keep Dropout active during the test phase. You must use the Functional API format.\n",
        "- Use the following pattern to create your model step by step:\n",
        "\n",
        "    - inputs = your_layer_1(arguments)(x)\n",
        "\n",
        "    - x = your_layer_2(arguments)(x)\n",
        "\n",
        "    - ...\n",
        "\n",
        "    - outputs = your_final_layer(arguments)(x)\n",
        "\n",
        "- For the architecture:\n",
        "\n",
        "    - Start with two or three convolutional layers with MaxPooling, then add a Flatten layer.\n",
        "\n",
        "    - Then move on to two or three Dense layers.\n",
        "\n",
        "    - The last Dense layer here will define the latent space. The number of neurons will determine the dimensionality of this latent space. Try using two neurons in the latent space for now. Name it \"latent_space\" (not x). Use a sigmoid activation function for this layer.\n",
        "\n",
        "    - Then build the decoder part, which is basically a mirror of the encoder.\n",
        "\n",
        "    - Recreate the Dense layers symmetrically.\n",
        "\n",
        "    - The last Dense layer of the decoder must have the same number of neurons as the output of the Flatten layer (you can use model.summary() to check this).\n",
        "\n",
        "    - Apply a reshape (keras.layers.Reshape()) to restore the shape of the data to what it was before the first Flatten layer.\n",
        "\n",
        "    - Then apply the mirror of your first convolutional layers, replacing Conv2D with Conv2DTranspose and MaxPooling2D with UpSampling2D (the inverse operation of MaxPooling: it increases the spatial dimensions).\n",
        "\n",
        "    - The output dimensions must match the input dimensions. This means that the final Conv2DTranspose layer must have only one neuron (one channel). Use a sigmoid activation function in the final layer (since the input data were normalized between 0 and 1)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "hnAJ7m_zLYbo"
      },
      "outputs": [],
      "source": [
        "#TO DO\n",
        "inputs = keras.layers.Input(shape = X_train.shape[1:])\n",
        "x = keras.layers.Conv2D(12, kernel_size=(3, 3), activation='relu')(inputs)\n",
        "x = keras.layers.MaxPooling2D(pool_size=(2, 2))(x)\n",
        "x = keras.layers.Conv2D(12, kernel_size=(3, 3), activation='relu')(x)\n",
        "x = keras.layers.MaxPooling2D(pool_size=(2, 2))(x)\n",
        "size_before_flatten = x.shape\n",
        "x = keras.layers.Flatten()(x)\n",
        "numneurons_flatten = x.shape[1]\n",
        "x = keras.layers.Dense(10, activation='relu')(x)\n",
        "x = keras.layers.Dense(10, activation='relu')(x)\n",
        "x = keras.layers.Dense(2, activation='sigmoid')(x)\n",
        "latent_space = x\n",
        "x = keras.layers.Dense(10, activation='relu')(latent_space)\n",
        "x = keras.layers.Dense(10, activation='relu')(x)\n",
        "x = keras.layers.Dense(numneurons_flatten , activation='sigmoid')(x)\n",
        "x = keras.layers.Reshape(size_before_flatten[1:])(x)\n",
        "x = keras.layers.UpSampling2D(size=(2, 2))(x)\n",
        "x = keras.layers.Conv2DTranspose(12, kernel_size=(3, 3), activation='relu')(x)\n",
        "x = keras.layers.UpSampling2D(size=(2, 2))(x)\n",
        "x = keras.layers.Conv2DTranspose(12, kernel_size=(3, 3), activation='relu')(x)\n",
        "outputs = keras.layers.Conv2DTranspose(1, kernel_size=(3, 3), activation='sigmoid')(x)\n",
        "\n",
        "\n",
        "#DO NOT CHANGE\n",
        "my_model = keras.models.Model(inputs,outputs) #Full model construction\n",
        "encoder = keras.models.Model(inputs,latent_space) #Encoder part\n",
        "decoder = keras.models.Model(latent_space,outputs) #Decoder part"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UkUCQ9bJLYbo"
      },
      "source": [
        "**Exercise**: Display your architecture by calling my_model.summary()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 753
        },
        "id": "chCSHRIXLYbp",
        "outputId": "45d798e8-9220-49b7-98b8-a86ddafd2bea"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1mModel: \"functional_3\"\u001b[0m\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"functional_3\"</span>\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
              "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
              "│ input_layer_6 (\u001b[38;5;33mInputLayer\u001b[0m)      │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m28\u001b[0m, \u001b[38;5;34m28\u001b[0m, \u001b[38;5;34m1\u001b[0m)      │             \u001b[38;5;34m0\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ conv2d_12 (\u001b[38;5;33mConv2D\u001b[0m)              │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m26\u001b[0m, \u001b[38;5;34m26\u001b[0m, \u001b[38;5;34m12\u001b[0m)     │           \u001b[38;5;34m120\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ max_pooling2d_12 (\u001b[38;5;33mMaxPooling2D\u001b[0m) │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m13\u001b[0m, \u001b[38;5;34m13\u001b[0m, \u001b[38;5;34m12\u001b[0m)     │             \u001b[38;5;34m0\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ conv2d_13 (\u001b[38;5;33mConv2D\u001b[0m)              │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m11\u001b[0m, \u001b[38;5;34m11\u001b[0m, \u001b[38;5;34m12\u001b[0m)     │         \u001b[38;5;34m1,308\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ max_pooling2d_13 (\u001b[38;5;33mMaxPooling2D\u001b[0m) │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m5\u001b[0m, \u001b[38;5;34m5\u001b[0m, \u001b[38;5;34m12\u001b[0m)       │             \u001b[38;5;34m0\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ flatten_6 (\u001b[38;5;33mFlatten\u001b[0m)             │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m300\u001b[0m)            │             \u001b[38;5;34m0\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dense_36 (\u001b[38;5;33mDense\u001b[0m)                │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m10\u001b[0m)             │         \u001b[38;5;34m3,010\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dense_37 (\u001b[38;5;33mDense\u001b[0m)                │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m10\u001b[0m)             │           \u001b[38;5;34m110\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dense_38 (\u001b[38;5;33mDense\u001b[0m)                │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m2\u001b[0m)              │            \u001b[38;5;34m22\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dense_39 (\u001b[38;5;33mDense\u001b[0m)                │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m10\u001b[0m)             │            \u001b[38;5;34m30\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dense_40 (\u001b[38;5;33mDense\u001b[0m)                │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m10\u001b[0m)             │           \u001b[38;5;34m110\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dense_41 (\u001b[38;5;33mDense\u001b[0m)                │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m300\u001b[0m)            │         \u001b[38;5;34m3,300\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ reshape_5 (\u001b[38;5;33mReshape\u001b[0m)             │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m5\u001b[0m, \u001b[38;5;34m5\u001b[0m, \u001b[38;5;34m12\u001b[0m)       │             \u001b[38;5;34m0\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ up_sampling2d_2 (\u001b[38;5;33mUpSampling2D\u001b[0m)  │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m10\u001b[0m, \u001b[38;5;34m10\u001b[0m, \u001b[38;5;34m12\u001b[0m)     │             \u001b[38;5;34m0\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ conv2d_transpose_4              │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m12\u001b[0m, \u001b[38;5;34m12\u001b[0m, \u001b[38;5;34m12\u001b[0m)     │         \u001b[38;5;34m1,308\u001b[0m │\n",
              "│ (\u001b[38;5;33mConv2DTranspose\u001b[0m)               │                        │               │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ up_sampling2d_3 (\u001b[38;5;33mUpSampling2D\u001b[0m)  │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m24\u001b[0m, \u001b[38;5;34m24\u001b[0m, \u001b[38;5;34m12\u001b[0m)     │             \u001b[38;5;34m0\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ conv2d_transpose_5              │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m26\u001b[0m, \u001b[38;5;34m26\u001b[0m, \u001b[38;5;34m12\u001b[0m)     │         \u001b[38;5;34m1,308\u001b[0m │\n",
              "│ (\u001b[38;5;33mConv2DTranspose\u001b[0m)               │                        │               │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ conv2d_transpose_6              │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m28\u001b[0m, \u001b[38;5;34m28\u001b[0m, \u001b[38;5;34m1\u001b[0m)      │           \u001b[38;5;34m109\u001b[0m │\n",
              "│ (\u001b[38;5;33mConv2DTranspose\u001b[0m)               │                        │               │\n",
              "└─────────────────────────────────┴────────────────────────┴───────────────┘\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
              "┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
              "│ input_layer_6 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)      │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">28</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">28</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>)      │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ conv2d_12 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)              │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">26</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">26</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">12</span>)     │           <span style=\"color: #00af00; text-decoration-color: #00af00\">120</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ max_pooling2d_12 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">MaxPooling2D</span>) │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">13</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">13</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">12</span>)     │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ conv2d_13 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)              │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">11</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">11</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">12</span>)     │         <span style=\"color: #00af00; text-decoration-color: #00af00\">1,308</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ max_pooling2d_13 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">MaxPooling2D</span>) │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">5</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">5</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">12</span>)       │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ flatten_6 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Flatten</span>)             │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">300</span>)            │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dense_36 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">10</span>)             │         <span style=\"color: #00af00; text-decoration-color: #00af00\">3,010</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dense_37 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">10</span>)             │           <span style=\"color: #00af00; text-decoration-color: #00af00\">110</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dense_38 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">2</span>)              │            <span style=\"color: #00af00; text-decoration-color: #00af00\">22</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dense_39 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">10</span>)             │            <span style=\"color: #00af00; text-decoration-color: #00af00\">30</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dense_40 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">10</span>)             │           <span style=\"color: #00af00; text-decoration-color: #00af00\">110</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dense_41 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">300</span>)            │         <span style=\"color: #00af00; text-decoration-color: #00af00\">3,300</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ reshape_5 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Reshape</span>)             │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">5</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">5</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">12</span>)       │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ up_sampling2d_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">UpSampling2D</span>)  │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">10</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">10</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">12</span>)     │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ conv2d_transpose_4              │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">12</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">12</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">12</span>)     │         <span style=\"color: #00af00; text-decoration-color: #00af00\">1,308</span> │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2DTranspose</span>)               │                        │               │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ up_sampling2d_3 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">UpSampling2D</span>)  │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">24</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">24</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">12</span>)     │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ conv2d_transpose_5              │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">26</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">26</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">12</span>)     │         <span style=\"color: #00af00; text-decoration-color: #00af00\">1,308</span> │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2DTranspose</span>)               │                        │               │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ conv2d_transpose_6              │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">28</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">28</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>)      │           <span style=\"color: #00af00; text-decoration-color: #00af00\">109</span> │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2DTranspose</span>)               │                        │               │\n",
              "└─────────────────────────────────┴────────────────────────┴───────────────┘\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m10,735\u001b[0m (41.93 KB)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">10,735</span> (41.93 KB)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m10,735\u001b[0m (41.93 KB)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">10,735</span> (41.93 KB)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        }
      ],
      "source": [
        "#TO DO\n",
        "my_model.summary()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wyKfVacqLYbp"
      },
      "source": [
        "**Verification**: The dimension of the last layer should be (None,28,28,1), the layer in the middle should be named \"latent_space\" and should contain 2 neurons."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vDsVoEs6LYbq"
      },
      "source": [
        "### Model compilation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ucfMewzhLYbq"
      },
      "source": [
        "**Exercise**: Compile your model and choose an optimizer. For an autoencoder, we will compare the output with the input image. We can use a mean squared error loss function, or a binary_crossentropy. It is not necessary to define any metric."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "QMua0DhALYbq"
      },
      "outputs": [],
      "source": [
        "opt = keras.optimizers.Adam(learning_rate=1e-4)\n",
        "my_model.compile(optimizer=opt, loss='binary_crossentropy')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A7GUDWL4LYbr"
      },
      "source": [
        "### Early stopping"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oWwWBhY0LYbr"
      },
      "source": [
        "**Exercise**: Define an early stopping procedure."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "ewaE15sjLYbr"
      },
      "outputs": [],
      "source": [
        "early_stopping = keras.callbacks.EarlyStopping(monitor='val_loss', patience=6, restore_best_weights=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "S8xv2zTsLYbr"
      },
      "source": [
        "## L'apprentissage"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PIezJaVOLYbs"
      },
      "source": [
        "**Exercise**: Run the training as usual. The output data should be X_train here!\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dZOP7TmHLYbs",
        "outputId": "725f0f18-0bf5-4fb5-ba89-817c072ea686"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/100\n",
            "\u001b[1m375/375\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m52s\u001b[0m 130ms/step - loss: 0.5852 - val_loss: 0.3801\n",
            "Epoch 2/100\n",
            "\u001b[1m375/375\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m47s\u001b[0m 125ms/step - loss: 0.3551 - val_loss: 0.3052\n",
            "Epoch 3/100\n"
          ]
        }
      ],
      "source": [
        "my_model.fit(X_train,X_train, epochs = 100, batch_size = 128, validation_split = 0.2, callbacks = [early_stopping])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AyhzI-fBLYbs"
      },
      "source": [
        "**Verification**: The loss function should decrease and the accuracy should increase. Same thing for the validation loss."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wrfnRoTZLYbs"
      },
      "source": [
        "**Exercise**: Plot the evolution of the loss function, and the evolution of the accuracy, for the training set and the validation set."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6S9e5PPkLYbs",
        "scrolled": true
      },
      "outputs": [],
      "source": [
        "#TO DO"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3NsNju9sLYbt"
      },
      "source": [
        "## Predicting with your model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hlXNTtoCLYbt"
      },
      "source": [
        "**Exercise**: Run the prediction on the test set. Give it the name X_pred_test."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "taRu59f7LYbt"
      },
      "outputs": [],
      "source": [
        "#TO DO"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aLXR_vdXNQKM"
      },
      "source": [
        "**Exercise**: Display an example of X_test and X_pred_test."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YXlhQcZlLYbt",
        "scrolled": true
      },
      "outputs": [],
      "source": [
        "i = 3\n",
        "\n",
        "plt.figure(figsize = (16,9))\n",
        "\n",
        "plt.subplot(121)\n",
        "plt.imshow(#A COMPLETER)\n",
        "plt.title(\"Original example\")\n",
        "\n",
        "plt.subplot(122)\n",
        "plt.imshow(#A COMPLETER)\n",
        "plt.title(\"Decoded example\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r9pFBIN0OxQG"
      },
      "source": [
        "You can see that the data are not perfectly recovered: the dimension of the latent space is not enough to recover all of the dataset, but it should be quiet correct. We will fix this problem later. For the moment, we study the latent space in more details."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1N4JXU8xOGuq"
      },
      "source": [
        "## Latent space representation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uD3bmEmbOMjj"
      },
      "source": [
        "**Exercise**: Retrieve the outputs of the latent space. To do this, apply the model called encoder."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MJTYIDTzOg8d"
      },
      "outputs": [],
      "source": [
        "latent_space_pred = #TO DO\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tZ5MFN93Pygm"
      },
      "source": [
        "**Exercise**: We choose 2 dimensions for the latent space in order to easily visualize it. Thanks to plt.scatter, plot the points corresponding to this latent space. Give it a color corresponding to their label, thanks to Y_test (keyword: c = Vector of colors)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tPQXRO0jQBZP"
      },
      "outputs": [],
      "source": [
        "plt.figure(figsize = (16,9))\n",
        "\n",
        "plt.scatter(#TO DO)\n",
        "plt.colorbar()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ACiyIXBYLYbu"
      },
      "source": [
        "You should see some clusters, they cannot be well separated, but clearly visible. The 0s and 1s should be quiet well separated from the other classes."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "q68qwHqFR4vZ"
      },
      "source": [
        "## Better reconstruction"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "q58nhF6GR8Zo"
      },
      "source": [
        "**Exercise**: Take your autoencoder architecture and change the dimension of the latent space. A latent space with 10 dimensions should be enough. Compile and run the training on this new model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lNNR7R--R7sq"
      },
      "outputs": [],
      "source": [
        "#TO DO"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z5iIiWqAUvl8"
      },
      "source": [
        "**Exercise**: Plot the losses."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VBB4kOMvU0mC"
      },
      "outputs": [],
      "source": [
        "#TO DO"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ofCT2Y3VU-j1"
      },
      "source": [
        "**Exercise**: Run the prediction on the test set."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kwt0CquVVS-j"
      },
      "outputs": [],
      "source": [
        "X_pred_test = #TO DO"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ixr41ylKVUSD"
      },
      "source": [
        "**Exercise**: Pick some decoded examples and compare them to the original images."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "W804BwaATulx"
      },
      "outputs": [],
      "source": [
        "#TO DO"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P0ChN2ToXIbR"
      },
      "source": [
        "### Image denoising"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ctFiTSQJXLVe"
      },
      "source": [
        "**Exercise**: We apply your autoencoder for image denoising. Select a test image. Add a gaussian noise (with np.random.randn()), indicate the dimension of the noise to apply. Use a standard deviation of 0.1 to start. Apply a function np.clip on this noisy image so that its intensity is between 0 and 1. Finally, apply your auto-encoder to this image. Then you can play by changing the standard deviation for instance."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RE-qPbIvXKjw"
      },
      "outputs": [],
      "source": [
        "i = 6\n",
        "\n",
        "image = X_test[i:(i+1)]\n",
        "\n",
        "image_noisy = #TO DO (apply a gaussian noise and apply a clipping between 0 and 1)\n",
        "\n",
        "image_denoised = #TO DO\n",
        "\n",
        "plt.figure(figsize = (16,9))\n",
        "\n",
        "plt.subplot(131)\n",
        "plt.imshow(#TO DO)\n",
        "plt.title(\"Original example\")\n",
        "\n",
        "plt.subplot(132)\n",
        "plt.imshow(#TO DO)\n",
        "plt.title(\"Noisy image\")\n",
        "\n",
        "plt.subplot(133)\n",
        "plt.imshow(#TO DO)\n",
        "plt.title(\"Denoised image\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D4s2FpejZYyC"
      },
      "source": [
        "### Interpolating in the latent space"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xtK-RSbbZplE"
      },
      "source": [
        "**Exercise**: Pick two images and apply the encoder part. Save the corresponding latent variable. Do not use predict, apply directly the encoder by using: encoder (image)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RXwyupGaZ4Wp"
      },
      "outputs": [],
      "source": [
        "i_1 = 0\n",
        "i_2 = 102\n",
        "\n",
        "image_1 = #TO DO\n",
        "image_2 = #TO DO\n",
        "\n",
        "lat_1 = #TO DO\n",
        "lat_2 = #TO DO"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_VQgkaDBaf4W"
      },
      "source": [
        "**Exercise**: Apply an interpolation between the two latent variable by using the following formula: $lat_{interp} = (1 - \\lambda lat_1) + \\lambda lat_2$ with $\\lambda$ between 0 and 1, and use 10 values of $\\lambda$."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YwOXi9PSaLhp"
      },
      "outputs": [],
      "source": [
        "lambd = #TO DO (create a tensor with 10 elements, from 0 to 1, by using tf.linspace)\n",
        "\n",
        "#IN THE FOLLOWING CODE, WE ADAPT THE DIMENSION OF THE VARIABLES\n",
        "lambd = tf.cast(lambd,dtype = \"float32\")\n",
        "lambd = tf.reshape(lambd,(10,1))\n",
        "lambd = tf.repeat(lambd,input_shape_dec[1],axis = 1)\n",
        "lat_1 = tf.repeat(lat_1,10,axis = 0)\n",
        "lat_2 = tf.repeat(lat_2,10,axis = 0)\n",
        "\n",
        "lat_interp = #TO DO (apply the interpolation formula)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2Dl-ZIFtejX1"
      },
      "source": [
        "**Exercise**: Run your decoder on the variable lat_interp."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "o0-mLlBHdQFN"
      },
      "outputs": [],
      "source": [
        "im_interp = #TO DO"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dCwkL4YMfHmn"
      },
      "source": [
        "Run the following cell to watch the interpolated images!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_XhLG0ulfETl"
      },
      "outputs": [],
      "source": [
        "plt.figure(figsize = (10,5))\n",
        "\n",
        "for i in range(10):\n",
        "  plt.subplot(2,5,i+1)\n",
        "  plt.imshow(im_interp[i,:,:,0])"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "name": "TP_Keras_MNIST_autoencoder_correction.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.5"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}