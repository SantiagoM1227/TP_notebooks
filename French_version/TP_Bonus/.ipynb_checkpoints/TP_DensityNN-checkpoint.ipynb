{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TP Programmation avec Keras - Density Neural Network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nous allons mettre en place un Density Neural Network afin d'effectuer une prédiction d'incertitudes.\n",
    "\n",
    "Dans ce TP, des cellules seront laissées à trous, il faudra les compléter suivant les consignes. Elles seront identifiées par le mot **Exercice**. Les **Vérifications** seront effectuées principalement par vous-mêmes, sur la bonne convergence des algorithmes ou leur bon fonctionnement."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ci-dessous, on importe les bibliothèques qui seront utiles."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow.keras as keras\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Mise en place des données"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Afin de pouvoir visualiser simplement, nous allons utiliser des données en une dimension (entrée et sortie) : un cosinus bruité avec un bruit gaussien. Ci-dessous, nous mettons en place les données."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#NE PAS MODIFIER\n",
    "N_train = 1000\n",
    "\n",
    "X_train = np.random.rand(N_train)*8 - 4\n",
    "\n",
    "sigma = np.abs(np.cos(X_train))*0.2\n",
    "\n",
    "Y_train = np.cos(X_train) + np.random.normal(0, sigma)\n",
    "\n",
    "N_test = 1000\n",
    "\n",
    "X_test = np.linspace(-8,8,N_test)\n",
    "\n",
    "Y_test = np.cos(X_test)\n",
    "\n",
    "X_train = np.reshape(X_train,(X_train.shape[0],1))\n",
    "\n",
    "Y_train = np.reshape(Y_train,(Y_train.shape[0],1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercice** : Visualisez les données d'entraînement avec un scatter plot."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#A COMPLETER"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Création d'une couche de densité et d'une loss function associée"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nous allons supposer que la sortie suite une loi de probabilité $p(y|x)$ normale de moyenne $\\mu$ et d'écart-type $\\sigma$. Nous allons chercher à prédire ces paramètres $\\mu$ et $\\sigma$. Pour ce faire, nous allons créer un type de couche adapté à leur prédiction.\n",
    "\n",
    "Ci-dessous, nous créons une couche DenseNormal qui prend entrée un nombre $N$ de neurones et produit $2N$ sorties : chaque dimension produit un $\\mu$ et un $\\sigma$.\n",
    "\n",
    "**Exerice** : $\\sigma$ doit être strictement positif. Pour ce faire, appliquez la fonction tf.nn.softplus à logsigma. Ajoutez en plus un petit $\\varepsilon$ pour éviter des divergences lors de l'apprentissage (éviter que $\\sigma$ ne s'approche trop de 0. 1e-6 devrait suffire."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DenseNormal(keras.layers.Layer):\n",
    "    def __init__(self, units):\n",
    "        super(DenseNormal, self).__init__()\n",
    "        self.units = int(units)\n",
    "        self.dense = keras.layers.Dense(2 * self.units)\n",
    "\n",
    "    def call(self, x):\n",
    "        output = self.dense(x)\n",
    "        mu, logsigma = tf.split(output, 2, axis=-1)\n",
    "        sigma = #A COMPLETER AVEC tf.nn.softplus et l'ajout d'un epsilon\n",
    "        return tf.concat([mu, sigma], axis=-1)\n",
    "\n",
    "    def compute_output_shape(self, input_shape):\n",
    "        return (input_shape[0], 2 * self.units)\n",
    "\n",
    "    def get_config(self):\n",
    "        base_config = super(DenseNormal, self).get_config()\n",
    "        base_config['units'] = self.units\n",
    "        return base_config"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nous allons créer ensuite la loss-function adaptée. Nous rappelons la vraisemblance de la loi gaussienne :\n",
    "\n",
    "\\begin{equation}\n",
    "p(y|\\mu,\\theta) = \\frac{1}{\\sqrt{2\\pi}\\sigma} e^{-\\frac{(y-\\mu)^2}{2\\sigma^2}}\n",
    "\\end{equation}\n",
    "\n",
    "La log-vraisemblance associée est donc donnée par :\n",
    "\n",
    "\\begin{equation}\n",
    "\\log(p(y|\\mu,\\theta)) = -\\log(\\sqrt{2\\pi}\\sigma) - (\\frac{(y-\\mu)^2}{2\\sigma^2})\n",
    "\\end{equation}\n",
    "\n",
    "**Exercice** : Complétez le code suivant en indiquant l'ensemble de la log-vraisemblance.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Gaussian_NLL(y, net_output, reduce=True):\n",
    "    mu, sigma = tf.split(net_output, 2, axis=-1)\n",
    "    ax = list(range(1, len(y.shape)))\n",
    "\n",
    "    logprob = #A COMPLETER\n",
    "    loss = tf.reduce_mean(-logprob, axis=ax)\n",
    "    return tf.reduce_mean(loss) if reduce else loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Modèle Keras"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Création du modèle"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercice** : Créez un modèle avec Keras que vous appellerez \"my_model\", uniquement fully_connected. La dernière couche sera une couche DenseNormal que vous avez créée, avec un seul neurone puisqu'on prédit une sortie à 1 dimension."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#A COMPLETER"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercice** : Affichez la structure de votre modèle avec my_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#A COMPLETER"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Vérification** : Pour l'instant, il suffit qu'il n'y ait pas d'erreur."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compilation du modèle"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercice** : Il faut maintenant compiler le modèle en utilisant la fonction de coût que vous avez définie au-dessus avec la negative log-likelihood de la distribution gaussienne. Il suffit d'entrer le nom de la fonction en loss (sans guillemets). Il ne sera pas utile de mettre de métrique."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#A COMPLETER"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Vérification** : De nouveau, s'il n'y a pas d'erreur et que vous avez suivi les instructions, tout devrait bien se passer."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## L'apprentissage"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercice** : Effectuez classiquement l'apprentissage."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learning = #A COMPLETER"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Vérification** : La loss function devrait diminuer."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercice** : Tracez l'évolution de la fonction de coût."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_evolution = learning.history[\"loss\"]\n",
    "\n",
    "\n",
    "plt.figure(figsize = (16,9))\n",
    "plt.plot(loss_evolution,label = \"Train set\")\n",
    "plt.xlabel(\"Epoques\")\n",
    "plt.ylabel(\"Valeur de la fonction de coût\")\n",
    "plt.legend()\n",
    "plt.title(\"Loss function evolution\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prédictions avec le modèle"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercice** : Effectuez une prédiction à partir de X_test."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y_pred_test = #A COMPLETER"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercice** : Séparez la prédiction en deux vecteurs mu et sigma."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mu = #A COMPLETER\n",
    "sigma = #A COMPLETER"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ci-dessous, voici la visualisation sur le jeu de test de votre prédiction (moyenne) et de l'incertitude associée. Les zones colorées correspondent aux incertitudes à 1, 2, 3 et 4 sigma."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "var = np.minimum(sigma, 1e3)\n",
    "    \n",
    "plt.figure(figsize=(5, 3), dpi=200)\n",
    "plt.title(\"Aleatoric uncertainty\")\n",
    "plt.scatter(X_train, Y_train, s=1., c='#463c3c', zorder=0, label=\"Train\")\n",
    "plt.plot(X_test, Y_test, 'r--', zorder=2, label=\"True\")\n",
    "plt.plot(X_test, mu, color='#007cab', zorder=3, label=\"Pred\")\n",
    "plt.plot([-4, -4], [-150, 150], 'k--', alpha=0.4, zorder=0)\n",
    "plt.plot([+4, +4], [-150, 150], 'k--', alpha=0.4, zorder=0)\n",
    "for k in np.linspace(0, 4, 4):\n",
    "    plt.fill_between(\n",
    "        X_test, (mu - k * var), (mu + k * var),\n",
    "        alpha=0.3,\n",
    "        edgecolor=None,\n",
    "        facecolor='#00aeef',\n",
    "        linewidth=0,\n",
    "        zorder=1,\n",
    "        label=\"Unc.\" if k == 0 else None)\n",
    "plt.gca().set_ylim(-2, 2)\n",
    "plt.gca().set_xlim(-7, 7)\n",
    "plt.legend(loc=\"upper left\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
