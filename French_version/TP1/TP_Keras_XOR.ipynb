{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TP Programmation avec Keras - Cas XOR"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nous allons produire un réseau de neurones pour effectuer une classification sur le cas XOR, en utilisant les bibliothèques tensorflow/keras.\n",
    "\n",
    "Dans ce TP, des cellules seront laissées à trous, il faudra les compléter suivant les consignes. Elles seront identifiées par le mot **Exercice**. Les **Vérifications** seront effectuées principalement par vous-mêmes, sur la bonne convergence des algorithmes ou leur bon fonctionnement."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ci-dessous, on importe les bibliothèques qui seront utiles."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow.keras as keras"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Mise en place des données"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nous allons nous intéresser au problème du XOR (ou exclusif). Prenons des vecteurs à deux dimensions. Si les deux coordonnées ont le même signe, on dira que le vecteur appartient à la classe 1, et 0 si les deux coordonnées sont de signes opposés. En exercice, vous pouvez faire un dessin représentant la situation. On comprend ici qu'un séparateur linéaire ne permettra pas de séparer les deux classes. Nous allons faire un réseau de neurones pour cela. Ci-dessous, je vous construis une base de données.\n",
    "\n",
    "Exécutez la cellule ci-dessous pour créer les données. N'hésitez pas à les visualiser."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#NE PAS MODIFIER\n",
    "\n",
    "N_train = 200\n",
    "\n",
    "X_train = 2*np.random.rand(N_train,2) - 1\n",
    "\n",
    "Y_train = (np.sign(X_train[:,0]) == np.sign(X_train[:,1]))*1\n",
    "\n",
    "Y_train = np.reshape(Y_train,(N_train,1))\n",
    "\n",
    "N_test = 100\n",
    "\n",
    "X_test = 2*np.random.rand(N_test,2) - 1\n",
    "\n",
    "Y_test = (np.sign(X_test[:,0]) == np.sign(X_test[:,1]))*1\n",
    "\n",
    "Y_test = np.reshape(Y_test,(N_test,1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Modèle Keras"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Création du modèle"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercice** : Créez un modèle avec Keras que vous appellerez \"my_model\". Construisez-le de sorte à ce qu'il ait 3 couches Dense (fully-connected) de 5, 5 et 1 neurones respectivement. Les 2 premières couches doivent avoir une fonction d'activation ReLU et celle de la dernière couche sera sigmoïde.\n",
    "\n",
    "**Hints** :\n",
    "- Initialisez le modèle avec keras.Sequential\n",
    "- Pour ajouter une couche, utilisez my_model.add(LAYER)\n",
    "- Vous trouverez les layers en utilisant keras.layers.Dense, il faut mettre en argument le nombre de neurones et la fonction d'activation.\n",
    "- Pour la première couche, précisez la taille de l'input avec le mot-clé input_shape : ici elle est de la forme (2,). Il faut bien mettre la virgule qui peut paraître inutile."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_model = #A COMPLETER\n",
    "\n",
    "#COMPLETEZ AVEC LA STRUCTURE EN SUIVANT LES INSTRUCTIONS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercice** : Affichez la structure de votre modèle avec my_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#A COMPLETER"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Vérification** : Pour l'instant, il suffit qu'il n'y ait pas d'erreur."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compilation du modèle"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercice** : Il faut maintenant compiler le modèle, en indiquant la fonction de coût à utiliser, les différentes métriques à utiliser, ainsi que l'optimizer que vous souhaitez utiliser ainsi que le learning rate ou encore d'autre paramètres propres à l'optimizer.\n",
    "\n",
    "**Hints** :\n",
    "- Définissez déjà l'optimizer dans la variable opt. Utilisez l'optimizer Adam que vous trouverez dans keras.optimizers.Adam. Précisez le learning rate avec le mot-clé learning_rate, et utilisez un learning rate de 0.01.\n",
    "- Utilisez ensuite my_model.compile en précisant :\n",
    "    - l'optimizer avec le mot-clé optimizer, et lui donnant la variable opt créée précédemment\n",
    "    - la fonction de coût avec le mot-clé loss. On utilise la binary cross entropy que l'on appelle avec la chaîne de caractères \"binary_crossentropy\"\n",
    "    - la métrique avec le mot-clé metrics. Il faut fournir une liste contenant les métriques d'intérêt. Ici utilisez une liste qui ne contient que la métrique \"binary_accuracy\" qui correspond au taux de bonnes réponses de votre réseau de neurones (avec un seuil de 0.5). Vous verrez cette métrique être évaluée au cours de l'apprentissage sur le jeu d'entraînement."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "opt = #A COMPLETER\n",
    "\n",
    "#COMPLETEZ PAR MY_MODEL.COMPILE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Vérification** : De nouveau, s'il n'y a pas d'erreur et que vous avez suivi les instructions, tout devrait bien se passer."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## L'apprentissage"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercice** : Il faut maintenant effectuer l'apprentissage !\n",
    "\n",
    "**Hints** :\n",
    "- Utilisez my_model.fit en donnant les arguments suivant :\n",
    "    - D'abord les entrées du jeu d'entraînement X_train\n",
    "    - Puis les sorties Y_train\n",
    "    - Le nombre d'époques avec le mot-clé epochs. Une centaine d'époques devraient suffire\n",
    "- Stockez l'apprentissage dans la variable learning (learning = model.fit(...)). Cela permettra a posteriori de récupérer des informations sur l'apprentissage (l'évolution de la loss ou de la métrique par exemple)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "learning = #A COMPLETER"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Vérification** : La loss function devrait diminuer et l'accuracy augmenter."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercice** : On va tracer l'évolution de la fonction de coût et de l'accuracy. Complétez le code ci-dessous. Vous trouverez la fonction de coût dans learning.history[\"loss\"] et l'accuracy dans learning.history[\"binary_accuracy\"]."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "loss_evolution = #A COMPLETER\n",
    "acc_evolution = #A COMPLETER\n",
    "\n",
    "plt.figure(figsize = (16,9))\n",
    "plt.subplot(121)\n",
    "plt.plot(loss_evolution)\n",
    "plt.xlabel(\"Epoques\")\n",
    "plt.ylabel(\"Valeur de la fonction de coût\")\n",
    "plt.title(\"Loss function evolution\")\n",
    "\n",
    "plt.subplot(122)\n",
    "plt.plot(acc_evolution)\n",
    "plt.xlabel(\"Epoques\")\n",
    "plt.ylabel(\"Valeur de l'accuracy\")\n",
    "plt.title(\"Binary accuracy evolution\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vous pourrez peut-être vous dire qu'on aurait pu continuer l'apprentissage, la fonction de coût peut sembler vouloir continuer de diminuer. Je vous inviterai à relancer votre modèle avec beaucoup plus d'époques (1000 ou même plus) et voir l'effet que cela a sur l'accuracy du jeu d'apprentissage, ainsi que sur votre jeu de test."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prédictions avec le modèle"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercice** : Nous allons maintenant faire une prédiction sur le jeu de test. Il suffit d'utiliser my_model.predict appliqué sur le jeu de test X_test. Stockez cette prédiction dans la variable Y_pred_test."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y_pred_test = #A COMPLETER"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ci dessous, vous trouverez la précision sur le jeu de test, ainsi que sur le jeu d'entraînement : on se fixe un seuil de 0.5, si la prédiction est supérieure à 0.5, on l'associe à la classe 1, et à la classe 0 sinon. On compte le nombre de fois où la classe prédite est égale à la classe attendue et on regarde par rapport au nombre d'exemples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print(\"Accuracy sur le jeu de test : \" + str(np.sum((Y_pred_test > 0.5) == Y_test)/N_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ci-dessous pour visualiser vos prédictions sur l'ensemble du carré unité en deux dimensions !"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grid = np.meshgrid(np.linspace(-1.,1.,50),np.linspace(-1,1,50))\n",
    "\n",
    "X_test_new = np.array([grid[0].flatten(),grid[1].flatten()])\n",
    "\n",
    "Y_pred_new = my_model.predict(X_test_new.T)\n",
    "\n",
    "maps = plt.imshow((Y_pred_new.reshape(grid[0].shape[0],grid[0].shape[1])),extent = (-1,1,-1,1),cmap = \"hot\",origin = \"lower\")\n",
    "plt.scatter(X_test[:,0],X_test[:,1],color = \"blue\",marker = \"x\")\n",
    "plt.colorbar(maps, label = \"Sortie du réseau de neurones\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
